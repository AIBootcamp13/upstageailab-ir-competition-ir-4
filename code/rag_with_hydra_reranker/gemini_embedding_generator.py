import os
import json
import logging
import numpy as np
import time
from pathlib import Path
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import hydra
from omegaconf import DictConfig

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class GeminiEmbeddingGenerator:
    def __init__(self, cfg):
        self.cfg = cfg
        self.log = logging.getLogger(__name__)

        # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = Path("gemini_embeddings")
        self.output_dir.mkdir(exist_ok=True)

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.metadata_file = self.output_dir / "metadata.json"

        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError("Gemini ì„ë² ë”© ì‚¬ìš©ì‹œ GOOGLE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.model = GoogleGenerativeAIEmbeddings(
            model=cfg.retrieve.dense_gemini.model_name,
            google_api_key=google_api_key
        )

        # ì„¤ì •ê°’
        self.batch_size = getattr(cfg.retrieve.dense_gemini, 'batch_size', 100)
        self.batch_delay = getattr(cfg.retrieve.dense_gemini, 'batch_delay_seconds', 65)

        self.log.info(f"Gemini ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self.log.info(f"ëª¨ë¸: {cfg.retrieve.dense_gemini.model_name}")
        self.log.info(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        self.log.info(f"ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„: {self.batch_delay}ì´ˆ")

    def load_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì§„í–‰ìƒí™©, ì„¤ì • ë“±)"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "last_completed_batch": -1,
                "total_documents": 0,
                "total_batches": 0,
                "batch_size": self.batch_size,
                "completed": False,
                "start_time": None,
                "last_update_time": None
            }

    def save_metadata(self, metadata):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata["last_update_time"] = time.time()
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def load_documents(self):
        """ë¬¸ì„œ ë¡œë“œ"""
        docs = []
        with open(self.cfg.paths.documents, 'r', encoding='utf-8') as f:
            for line in f:
                docs.append(json.loads(line))
        return docs

    def save_batch_embeddings(self, batch_idx, embeddings):
        """ë°°ì¹˜ ì„ë² ë”© ì €ì¥"""
        batch_file = self.output_dir / f"batch_{batch_idx:04d}.npy"
        np.save(batch_file, embeddings)
        self.log.info(f"ë°°ì¹˜ {batch_idx} ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {batch_file}")

    def load_batch_embeddings(self, batch_idx):
        """ë°°ì¹˜ ì„ë² ë”© ë¡œë“œ"""
        batch_file = self.output_dir / f"batch_{batch_idx:04d}.npy"
        if batch_file.exists():
            return np.load(batch_file)
        return None

    def is_rate_limit_error(self, error):
        """Rate limit ë˜ëŠ” quota ì˜¤ë¥˜ì¸ì§€ í™•ì¸"""
        error_str = str(error).lower()
        rate_limit_keywords = [
            "rate limit", "quota", "too many requests",
            "resource_exhausted", "429", "quota exceeded"
        ]
        return any(keyword in error_str for keyword in rate_limit_keywords)

    def generate_embeddings(self):
        """ì„ë² ë”© ìƒì„± (ì´ì–´ë°›ê¸° ì§€ì›)"""
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata = self.load_metadata()

        # ë¬¸ì„œ ë¡œë“œ
        docs = self.load_documents()
        total_docs = len(docs)
        total_batches = (total_docs + self.batch_size - 1) // self.batch_size

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if metadata["start_time"] is None:
            metadata["start_time"] = time.time()
        metadata["total_documents"] = total_docs
        metadata["total_batches"] = total_batches
        metadata["batch_size"] = self.batch_size

        start_batch = metadata["last_completed_batch"] + 1

        if start_batch == 0:
            self.log.info(f"ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± ì‹œì‘: ì´ {total_docs}ê°œ ë¬¸ì„œ, {total_batches}ê°œ ë°°ì¹˜")
        else:
            self.log.info(f"ì„ë² ë”© ìƒì„± ì¬ê°œ: ë°°ì¹˜ {start_batch}ë¶€í„° ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")

        try:
            for batch_idx in range(start_batch, total_batches):
                batch_start_time = time.time()

                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, total_docs)
                batch_docs = docs[start_idx:end_idx]
                contents = [doc["content"] for doc in batch_docs]

                self.log.info(f"[{batch_idx+1}/{total_batches}] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... (ë¬¸ì„œ {start_idx+1}-{end_idx}/{total_docs})")

                # ì„ë² ë”© ìƒì„±
                embeddings = self.model.embed_documents(contents)

                # ë°°ì¹˜ ì €ì¥
                self.save_batch_embeddings(batch_idx, embeddings)

                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                metadata["last_completed_batch"] = batch_idx
                self.save_metadata(metadata)

                batch_time = time.time() - batch_start_time
                progress = (batch_idx + 1) / total_batches * 100

                # ì§„í–‰ìƒí™© ë¡œê·¸
                if batch_idx > start_batch:
                    elapsed_time = time.time() - metadata["start_time"]
                    avg_batch_time = elapsed_time / (batch_idx - start_batch + 1)
                    remaining_batches = total_batches - (batch_idx + 1)
                    eta_seconds = remaining_batches * avg_batch_time
                    eta_minutes = eta_seconds / 60
                    self.log.info(f"[{batch_idx+1}/{total_batches}] ì™„ë£Œ ({progress:.1f}%) - ë°°ì¹˜ ì²˜ë¦¬ì‹œê°„: {batch_time:.1f}ì´ˆ, ì˜ˆìƒ ë‚¨ì€ì‹œê°„: {eta_minutes:.1f}ë¶„")
                else:
                    self.log.info(f"[{batch_idx+1}/{total_batches}] ì™„ë£Œ ({progress:.1f}%) - ë°°ì¹˜ ì²˜ë¦¬ì‹œê°„: {batch_time:.1f}ì´ˆ")

                # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸° (rate limit íšŒí”¼)
                if batch_idx + 1 < total_batches:
                    self.log.info(f"Rate limit íšŒí”¼ë¥¼ ìœ„í•´ {self.batch_delay}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(self.batch_delay)

        except Exception as e:
            if self.is_rate_limit_error(e):
                self.log.warning(f"Rate limit ë˜ëŠ” quota ë„ë‹¬ë¡œ ì¤‘ë‹¨ë¨: {e}")
                self.log.info(f"í˜„ì¬ê¹Œì§€ {metadata['last_completed_batch'] + 1}ê°œ ë°°ì¹˜ ì™„ë£Œ")
                self.log.info("ì œí•œ í•´ì œ í›„ ë™ì¼í•œ ëª…ë ¹ìœ¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•˜ì‹œë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
            else:
                self.log.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return False

        # ëª¨ë“  ë°°ì¹˜ ì™„ë£Œ
        metadata["completed"] = True
        metadata["completion_time"] = time.time()
        self.save_metadata(metadata)

        total_time = time.time() - metadata["start_time"]
        self.log.info(f"âœ… ëª¨ë“  ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        self.log.info(f"ì´ {total_docs}ê°œ ë¬¸ì„œ, {total_batches}ê°œ ë°°ì¹˜, ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")

        # ë³‘í•©ëœ íŒŒì¼ ìƒì„±
        self.merge_all_batches()
        return True

    def merge_all_batches(self):
        """ëª¨ë“  ë°°ì¹˜ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©"""
        metadata = self.load_metadata()
        if not metadata["completed"]:
            self.log.warning("ì•„ì§ ëª¨ë“  ë°°ì¹˜ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        self.log.info("ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘í•©í•˜ëŠ” ì¤‘...")
        all_embeddings = []

        for batch_idx in range(metadata["total_batches"]):
            embeddings = self.load_batch_embeddings(batch_idx)
            if embeddings is not None:
                all_embeddings.extend(embeddings)
            else:
                self.log.error(f"ë°°ì¹˜ {batch_idx} ë¡œë“œ ì‹¤íŒ¨")
                return

        # ë³‘í•©ëœ íŒŒì¼ ì €ì¥
        merged_file = self.output_dir / "all_embeddings.npy"
        np.save(merged_file, all_embeddings)

        self.log.info(f"ë³‘í•© ì™„ë£Œ: {len(all_embeddings)}ê°œ ì„ë² ë”© -> {merged_file}")

    def get_all_embeddings(self):
        """ëª¨ë“  ì„ë² ë”© ë°˜í™˜ (ë³‘í•©ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°°ì¹˜ë³„ë¡œ ë¡œë“œ)"""
        merged_file = self.output_dir / "all_embeddings.npy"

        if merged_file.exists():
            self.log.info(f"ë³‘í•©ëœ ì„ë² ë”© íŒŒì¼ ë¡œë“œ: {merged_file}")
            return np.load(merged_file)

        # ë°°ì¹˜ë³„ë¡œ ë¡œë“œ
        metadata = self.load_metadata()
        if not metadata["completed"]:
            self.log.error("ì„ë² ë”© ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. gemini_embedding_generator.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None

        self.log.info("ë°°ì¹˜ë³„ ì„ë² ë”© íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        all_embeddings = []

        for batch_idx in range(metadata["total_batches"]):
            embeddings = self.load_batch_embeddings(batch_idx)
            if embeddings is not None:
                all_embeddings.extend(embeddings)
            else:
                self.log.error(f"ë°°ì¹˜ {batch_idx} ë¡œë“œ ì‹¤íŒ¨")
                return None

        return np.array(all_embeddings)

@hydra.main(config_path="conf", config_name="config", version_base=None)
def main(cfg: DictConfig) -> None:
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    log = logging.getLogger(__name__)
    log.info("Gemini ì„ë² ë”© ìƒì„±ê¸° ì‹œì‘")

    try:
        generator = GeminiEmbeddingGenerator(cfg)
        success = generator.generate_embeddings()

        if success:
            log.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            log.info("â¸ï¸  ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹¤í–‰í•˜ì‹œë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")

    except Exception as e:
        log.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)

if __name__ == "__main__":
    main()