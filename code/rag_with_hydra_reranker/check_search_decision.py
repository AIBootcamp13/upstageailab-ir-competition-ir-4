import os
import json
import logging
from dotenv import load_dotenv
from openai import OpenAI
import traceback
import hydra
from omegaconf import DictConfig

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# Function callingì— ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜
def get_tools(cfg):
    return [
        {
            "type": "function",
            "function": {
                "name": "search",
                "description": "search relevant documents",
                "parameters": {
                    "properties": {
                        "standalone_query": {
                            "type": "string",
                            "description": cfg.prompts.standalone_query_description
                        }
                    },
                    "required": ["standalone_query"],
                    "type": "object"
                }
            }
        },
    ]

# LLMì—ê²Œ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê²Œ í•˜ëŠ” í•¨ìˆ˜
def check_search_decision(messages, client, cfg):
    """
    ì£¼ì–´ì§„ ë©”ì‹œì§€ì— ëŒ€í•´ LLMì´ ê²€ìƒ‰ì„ í˜¸ì¶œí• ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨

    Args:
        messages: ì‚¬ìš©ì ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        client: OpenAI í´ë¼ì´ì–¸íŠ¸
        cfg: Hydra config ê°ì²´

    Returns:
        dict: {
            "search_called": bool,
            "standalone_query": str or None
        }
    """
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (config.yamlì˜ cfg.prompts.function_calling ì‚¬ìš©)
    system_prompt = cfg.prompts.function_calling

    msg = [{"role": "system", "content": system_prompt}] + messages

    try:
        result = client.chat.completions.create(
            model=cfg.model.name,  # configì—ì„œ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            messages=msg,
            tools=get_tools(cfg),
            temperature=cfg.model.temperature,
            seed=cfg.model.seed,
            timeout=cfg.model.timeout,
            reasoning_effort=cfg.model.reasoning_effort
        )

        # LLM ë‹µë³€ ë‚´ìš© ìº¡ì²˜
        assistant_response = result.choices[0].message.content or ""

        # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° tool_callsê°€ ìˆìŒ
        if result.choices[0].message.tool_calls:
            tool_call = result.choices[0].message.tool_calls[0]
            function_args = json.loads(tool_call.function.arguments)
            standalone_query = function_args.get("standalone_query", "")

            return {
                "search_called": True,
                "standalone_query": standalone_query,
                "assistant_response": assistant_response
            }
        else:
            return {
                "search_called": False,
                "standalone_query": None,
                "assistant_response": assistant_response
            }
            
    except Exception as e:
        log.error(f"Error in check_search_decision: {e}")
        traceback.print_exc()
        return {
            "search_called": False,
            "standalone_query": None,
            "assistant_response": None,
            "error": str(e)
        }

def load_eval_data(eval_filename):
    """eval.jsonl íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
    eval_data = {}
    with open(eval_filename, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            eval_data[data['eval_id']] = data['msg']
    return eval_data

def check_search_decisions_for_eval_ids(cfg: DictConfig, eval_ids, eval_filename="../../input/data/eval.jsonl", output_filename=None):
    """
    ì§€ì •ëœ eval_idë“¤ì— ëŒ€í•´ ê²€ìƒ‰ í˜¸ì¶œ ì—¬ë¶€ë¥¼ í™•ì¸

    Args:
        cfg: Hydra config ê°ì²´
        eval_ids: í™•ì¸í•  eval_id ë¦¬ìŠ¤íŠ¸
        eval_filename: eval.jsonl íŒŒì¼ ê²½ë¡œ
        output_filename: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ ì¶œë ¥ë§Œ)

    Returns:
        list: ê° eval_idì— ëŒ€í•œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # OpenAI API í‚¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY environment variable is required")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    openai_base_url = os.getenv("OPENAI_BASE_URL")
    if openai_base_url:
        client = OpenAI(base_url=openai_base_url)
    else:
        client = OpenAI()

    # eval ë°ì´í„° ë¡œë“œ
    eval_data = load_eval_data(eval_filename)

    results = []

    for eval_id in eval_ids:
        if eval_id not in eval_data:
            log.warning(f"eval_id {eval_id} not found in eval data")
            continue

        messages = eval_data[eval_id]

        # ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ user ë©”ì‹œì§€)
        user_question = ""
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_question = msg["content"]
                break

        log.info(f"Processing eval_id {eval_id}: {user_question[:100]}...")

        # ê²€ìƒ‰ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸ (config ì „ë‹¬)
        decision = check_search_decision(messages, client, cfg)

        result = {
            "eval_id": eval_id,
            "question": user_question,
            "search_called": decision["search_called"],
            "standalone_query": decision["standalone_query"],
            "assistant_response": decision.get("assistant_response")
        }

        if "error" in decision:
            result["error"] = decision["error"]

        results.append(result)

        log.info(f"eval_id {eval_id}: search_called={decision['search_called']}, query='{decision.get('standalone_query', 'N/A')}'")

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ê²€ìƒ‰ í˜¸ì¶œ ì—¬ë¶€ ê²°ê³¼ ===")
    search_called_count = 0
    for result in results:
        status = "ğŸ” ê²€ìƒ‰ í˜¸ì¶œ" if result["search_called"] else "ğŸ’¬ ì§ì ‘ ë‹µë³€"
        print(f"\neval_id {result['eval_id']}: {status}")
        print(f"ì§ˆë¬¸: {result['question']}")
        if result["search_called"]:
            search_called_count += 1
            print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {result['standalone_query']}")
        if result.get("assistant_response"):
            print(f"LLM ë‹µë³€: {result['assistant_response']}")

    print(f"\nì´ {len(results)}ê°œ ì§ˆë¬¸ ì¤‘ {search_called_count}ê°œê°€ ê²€ìƒ‰ í˜¸ì¶œ, {len(results) - search_called_count}ê°œê°€ ì§ì ‘ ë‹µë³€")

    # íŒŒì¼ë¡œ ì €ì¥
    if output_filename:
        with open(output_filename, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        log.info(f"Results saved to {output_filename}")

    return results

@hydra.main(config_path="conf", config_name="config", version_base=None)
def main(cfg: DictConfig) -> None:
    """ë©”ì¸ í•¨ìˆ˜ - ì˜ˆì‹œ ì‚¬ìš©ë²•"""
    # ì˜ˆì‹œ: ëª‡ ê°œì˜ eval_idë¡œ í…ŒìŠ¤íŠ¸
    test_eval_ids = [66, 17]

    print("=== Search Decision Checker ===")
    print(f"í…ŒìŠ¤íŠ¸í•  eval_id: {test_eval_ids}")

    try:
        results = check_search_decisions_for_eval_ids(
            cfg=cfg,
            eval_ids=test_eval_ids,
            # output_filename="search_decision_results.jsonl"
        )

        print(f"\nì²˜ë¦¬ ì™„ë£Œ! ì´ {len(results)}ê°œ ê²°ê³¼")

    except Exception as e:
        log.error(f"Error in main: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
