import os
import json
import logging
from dotenv import load_dotenv
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from langchain_upstage import UpstageEmbeddings
import hydra
from omegaconf import DictConfig
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Sentence Transformer ëª¨ë¸ - main í•¨ìˆ˜ì—ì„œ ì´ˆê¸°í™”


# UpstageEmbeddingsë¥¼ ì´ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
def get_embedding(sentences, model, is_query=False):
    if is_query:
        return model.embed_query(sentences[0]) if len(sentences) == 1 else [model.embed_query(q) for q in sentences]
    else:
        return model.embed_documents(sentences)


# ë¬¸ì„œ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (passage embedding)
def get_embeddings_in_batches(docs, model, batch_size=100):
    log = logging.getLogger(__name__)
    batch_embeddings = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        contents = [doc["content"] for doc in batch]
        embeddings = get_embedding(contents, model, is_query=False)
        batch_embeddings.extend(embeddings)
        log.info(f'Processing batch {i}')
    return batch_embeddings


# ìƒˆë¡œìš´ index ìƒì„±
def create_es_index(es, index, settings, mappings):
    # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if es.indices.exists(index=index):
        # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì„¤ì •ì„ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ ê°±ì‹ í•˜ê¸° ìœ„í•´ ì‚­ì œ
        es.indices.delete(index=index)
    # ì§€ì •ëœ ì„¤ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
    es.indices.create(index=index, settings=settings, mappings=mappings)


# ì§€ì •ëœ ì¸ë±ìŠ¤ ì‚­ì œ
def delete_es_index(es, index):
    es.indices.delete(index=index)


# Elasticsearch í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ ì¸ë±ì‹± ìˆ˜í–‰
def bulk_add(es, index, docs):
    # ëŒ€ëŸ‰ ì¸ë±ì‹± ì‘ì—…ì„ ì¤€ë¹„
    actions = [
        {
            '_index': index,
            '_source': doc
        }
        for doc in docs
    ]
    return helpers.bulk(es, actions)


# ì—­ìƒ‰ì¸ì„ ì´ìš©í•œ ê²€ìƒ‰
def sparse_retrieve(es, index_name, query_str, size):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    return es.search(index=index_name, query=query, size=size, sort="_score")


# Vector ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ê²€ìƒ‰ (query embedding)
def dense_retrieve(es, model, index_name, query_str, size, num_candidates=100):
    # 4096ì°¨ì›ì˜ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = get_embedding([query_str], model, is_query=True)
    
    # 768ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ì¸ë±ì‹± ë•Œì™€ ë™ì¼í•˜ê²Œ)
    query_embedding_reduced = query_embedding[:768]
    
    if hasattr(query_embedding_reduced, 'tolist'):
        query_embedding_reduced = query_embedding_reduced.tolist()

    knn = {
        "field": "embeddings",
        "query_vector": query_embedding_reduced, # ì¶•ì†Œëœ ë²¡í„° ì‚¬ìš©
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=index_name, knn=knn)


# Qwen3-Reranker-8B ëª¨ë¸ ì´ˆê¸°í™”
def initialize_reranker(cfg):
    log = logging.getLogger(__name__)
    if not cfg.reranker.use_reranker:
        return None, None
    
    try:
        log.info(f"Loading reranker model: {cfg.reranker.model_name}")
        
        # PyTorch ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            log.info("PyTorch GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        tokenizer = AutoTokenizer.from_pretrained(cfg.reranker.model_name)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì§ì ‘ GPUì—ì„œ float16ìœ¼ë¡œ ë¡œë“œ
        if torch.cuda.is_available():
            log.info("ì§ì ‘ GPUì—ì„œ float16ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ...")
            model = AutoModelForSequenceClassification.from_pretrained(
                cfg.reranker.model_name, 
                torch_dtype=torch.float16,
            ).to('cuda').eval()
            log.info(f"Reranker model loaded on GPU with float16 precision")
            log.info(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
        else:
            # CPU fallback
            log.info("GPU ì‚¬ìš© ë¶ˆê°€, CPUì—ì„œ ëª¨ë¸ ë¡œë“œ...")
            model = AutoModelForSequenceClassification.from_pretrained(cfg.reranker.model_name).eval()
            log.info("Reranker model loaded on CPU")
        
        return tokenizer, model
    except Exception as e:
        log.error(f"Failed to load reranker model: {e}")
        return None, None


# ë¬¸ì„œë“¤ì„ rerankingí•˜ì—¬ ìƒìœ„ ë¬¸ì„œë“¤ ë°˜í™˜
def rerank_documents(query, documents, reranker_tokenizer, reranker_model, cfg):
    log = logging.getLogger(__name__)
    
    if reranker_tokenizer is None or reranker_model is None:
        log.warning("Reranker not initialized, returning original order")
        return documents[:cfg.reranker.top_k]
    
    try:
        # ì¿¼ë¦¬ì™€ ê° ë¬¸ì„œ ë‚´ìš©ì„ ìŒìœ¼ë¡œ êµ¬ì„±
        pairs = [[query, doc["content"]] for doc in documents]
        
        # ê° ë¬¸ì„œì— ëŒ€í•´ relevance score ê³„ì‚°
        relevance_scores = []
        
        with torch.no_grad():
            for i in range(0, len(pairs), cfg.reranker.batch_size):
                batch_pairs = pairs[i:i + cfg.reranker.batch_size]
                
                # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
                inputs = reranker_tokenizer(
                    batch_pairs,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to('cuda' if torch.cuda.is_available() else 'cpu')
                
                # ëª¨ë¸ì—ì„œ ì ìˆ˜(logits) ì§ì ‘ ì–»ê¸°
                scores = reranker_model(**inputs, return_dict=True).logits.view(-1)
                relevance_scores.extend(scores.cpu().tolist())
        
        # ë¬¸ì„œì™€ ì ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì •ë ¬
        doc_score_pairs = list(zip(documents, relevance_scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ top_k ë¬¸ì„œ ë°˜í™˜
        reranked_docs = [doc for doc, _ in doc_score_pairs[:cfg.reranker.top_k]]
        
        log.info(f"Reranked {len(documents)} documents to top {len(reranked_docs)}")
        return reranked_docs
        
    except Exception as e:
        log.error(f"Error during reranking: {e}")
        return documents[:cfg.reranker.top_k]


# Elasticsearch ì„¤ì •ì€ main í•¨ìˆ˜ì—ì„œ ì´ˆê¸°í™”

# Elasticsearch ì„¤ì •ë“¤ì€ main í•¨ìˆ˜ì—ì„œ ì •ì˜

# ì´ˆê¸°í™” ì½”ë“œë“¤ì€ main í•¨ìˆ˜ë¡œ ì´ë™

# RAGë¥¼ êµ¬í˜„í•˜ëŠ” ì½”ë“œ
from openai import OpenAI
import traceback

# OpenAI ì„¤ì •ë“¤ì€ main í•¨ìˆ˜ì—ì„œ ì´ˆê¸°í™”

# í”„ë¡¬í”„íŠ¸ë“¤ì€ config.yamlì—ì„œ ê´€ë¦¬

# Function callingì— ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜
def get_tools(cfg):
    return [
        {
            "type": "function",
            "function": {
                "name": "search",
                "description": "search relevant documents",
                "parameters": {
                    "properties": {
                        "standalone_query": {
                            "type": "string",
                            "description": cfg.prompts.standalone_query_description
                        }
                    },
                    "required": ["standalone_query"],
                    "type": "object"
                }
            }
        },
    ]


# LLMê³¼ ê²€ìƒ‰ì—”ì§„ì„ í™œìš©í•œ RAG êµ¬í˜„
def answer_question(messages, client, cfg, es, index_name, dense_model=None, reranker_tokenizer=None, reranker_model=None):
    # í•¨ìˆ˜ ì¶œë ¥ ì´ˆê¸°í™”
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    # ì§ˆì˜ ë¶„ì„ ë° ê²€ìƒ‰ ì´ì™¸ì˜ ì§ˆì˜ ëŒ€ì‘ì„ ìœ„í•œ LLM í™œìš©
    msg = [{"role": "system", "content": cfg.prompts.function_calling}] + messages
    try:
        result = client.chat.completions.create(
            model=cfg.model.name,
            messages=msg,
            tools=get_tools(cfg),
            #tool_choice={"type": "function", "function": {"name": "search"}},
            temperature=cfg.model.temperature,
            seed=cfg.model.seed,
            timeout=cfg.model.timeout,
            reasoning_effort=cfg.model.reasoning_effort
        )
    except Exception:
        traceback.print_exc()
        return response

    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ê²€ìƒ‰ í˜¸ì¶œí›„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    if result.choices[0].message.tool_calls:
        tool_call = result.choices[0].message.tool_calls[0]
        function_args = json.loads(tool_call.function.arguments)
        standalone_query = function_args.get("standalone_query")

        # Hybrid retrieval: sparse + dense
        sparse_result = sparse_retrieve(es, index_name, standalone_query, cfg.search.sparse.top_k)
        dense_result = dense_retrieve(es, dense_model, index_name, standalone_query, cfg.search.dense.top_k, cfg.search.dense.num_candidates)

        response["standalone_query"] = standalone_query
        
        # ê²°ê³¼ í•©ì¹˜ê¸° (ì¤‘ë³µ docid ì œê±°)
        docids = set()
        documents = []
        for rst in sparse_result['hits']['hits']:
            docid = rst["_source"]["docid"]
            if docid not in docids:
                docids.add(docid)
                documents.append({
                    "content": rst["_source"]["content"],
                    "docid": docid,
                    "score": rst["_score"]
                })
        for rst in dense_result['hits']['hits']:
            docid = rst["_source"]["docid"]
            if docid not in docids:
                docids.add(docid)
                documents.append({
                    "content": rst["_source"]["content"],
                    "docid": docid,
                    "score": rst["_score"]
                })
        
        # Rerankerê°€ í™œì„±í™”ëœ ê²½ìš° reranking ìˆ˜í–‰
        if cfg.reranker.use_reranker and reranker_tokenizer is not None and reranker_model is not None:
            reranked_documents = rerank_documents(standalone_query, documents, reranker_tokenizer, reranker_model, cfg)
        else:
            # Rerankerê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ìƒìœ„ top_kê°œë§Œ ì„ íƒ
            reranked_documents = documents[:cfg.reranker.top_k]
        
        # ìµœì¢… ê²°ê³¼ë¥¼ responseì— ì €ì¥
        retrieved_context = []
        for doc in reranked_documents:
            retrieved_context.append(doc["content"])
            response["topk"].append(doc["docid"])
            response["references"].append({"score": doc["score"], "content": doc["content"]})

        if cfg.qa.use_final_answer:
            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³„ë„ QA ìˆ˜í–‰
            content = json.dumps(retrieved_context)
            messages.append({"role": "assistant", "content": content})
            msg = [{"role": "system", "content": cfg.prompts.qa}] + messages
            try:
                qaresult = client.chat.completions.create(
                        model=cfg.model.name,
                        messages=msg,
                        temperature=cfg.model.temperature,
                        seed=cfg.model.seed,
                        timeout=cfg.model.qa_timeout
                    )
            except Exception:
                traceback.print_exc()
                return response
            response["answer"] = qaresult.choices[0].message.content
        else:
            # í˜„ì¬ ë°©ì‹: ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
            response["answer"] = result.choices[0].message.content

    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ë°”ë¡œ ë‹µë³€ ìƒì„±
    else:
        response["answer"] = result.choices[0].message.content

    return response


# í‰ê°€ë¥¼ ìœ„í•œ íŒŒì¼ì„ ì½ì–´ì„œ ê° í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ì¶”ì¶œí›„ íŒŒì¼ì— ì €ì¥
def eval_rag(eval_filename, output_filename, client, cfg, es, index_name, dense_model=None, reranker_tokenizer=None, reranker_model=None):
    log = logging.getLogger(__name__)
    general_questions = []  # ì¼ë°˜ì§ˆë¬¸ eval_id, answer ì €ì¥ ë¦¬ìŠ¤íŠ¸
    general_eval_ids = []   # eval_idë§Œ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    with open(eval_filename) as f, open(output_filename, "w") as of:
        idx = 0
        for line in f:
            j = json.loads(line)
            log.info(f'Test {idx} - Question: {j["msg"]}')
            response = answer_question(j["msg"], client, cfg, es, index_name, dense_model, reranker_tokenizer, reranker_model)
            log.info(f'Answer: {response["answer"]}')
            log.info(f'Retrieved {"ğŸ‘†ì¼ë°˜ì§ˆë¬¸ğŸ‘†" if len(response["topk"]) == 0 else len(response["topk"])} documents: {response["topk"]}')
            log.debug(f'References: {len(response["references"])} items')

            # ì¼ë°˜ì§ˆë¬¸ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            if len(response["topk"]) == 0:
                general_questions.append({"eval_id": j["eval_id"], "answer": response["answer"]})
                general_eval_ids.append(j["eval_id"])

            # ëŒ€íšŒ score ê³„ì‚°ì€ topk ì •ë³´ë¥¼ ì‚¬ìš©, answer ì •ë³´ëŠ” LLMì„ í†µí•œ ìë™í‰ê°€ì‹œ í™œìš©
            output = {"eval_id": j["eval_id"], "standalone_query": response["standalone_query"], "topk": response["topk"], "answer": response["answer"], "references": response["references"]}
            of.write(f'{json.dumps(output, ensure_ascii=False)}\n')
            idx += 1

            if cfg.eval.max_iterations > 0 and idx >= cfg.eval.max_iterations:
                break

    # ì¼ë°˜ì§ˆë¬¸ eval_id ë¦¬ìŠ¤íŠ¸ì™€ ê°¯ìˆ˜ ë¡œê·¸ì— ì¶œë ¥
    log.info(f'ì¼ë°˜ì§ˆë¬¸ eval_id ë¦¬ìŠ¤íŠ¸ ({len(general_eval_ids)}ê°œ): {general_eval_ids}')

    # chit_chat_idsë¥¼ ì œì™¸í•œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë¡œê·¸ì— ì¶œë ¥
    chit_chat_ids = {2, 32, 57, 64, 67, 83, 90, 94, 103, 218, 220, 222, 227, 229, 245, 247, 261, 276, 283, 301}
    filtered_eval_ids = [eid for eid in general_eval_ids if eid not in chit_chat_ids]
    log.info(f'chit_chat_ids ì œì™¸ ìµœì¢… eval_id ë¦¬ìŠ¤íŠ¸ ({len(filtered_eval_ids)}ê°œ): {filtered_eval_ids}')

@hydra.main(config_path="conf", config_name="config", version_base=None)
def main(cfg: DictConfig) -> None:
    log = logging.getLogger(__name__)
    log.info("Starting RAG evaluation process")
    
    # OpenAI API í‚¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY environment variable is required")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    openai_base_url = os.getenv("OPENAI_BASE_URL")
    if openai_base_url:
        client = OpenAI(base_url=openai_base_url)
    else:
        client = OpenAI()

    # Elasticsearch ì„¤ì •
    es_password = os.getenv("ELASTICSEARCH_PASSWORD")
    if not es_password:
        raise ValueError("ELASTICSEARCH_PASSWORD environment variable is required")

    # Elasticsearch client ìƒì„±
    es = Elasticsearch(
        list(cfg.elasticsearch.hosts), 
        basic_auth=(cfg.elasticsearch.username, es_password), 
        ca_certs=cfg.elasticsearch.ca_certs
    )
    log.info(f'Elasticsearch connection established: {es.info()}')

    # Sentence Transformer ëª¨ë¸ ì´ˆê¸°í™” (backward compatibility) - í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    # model = SentenceTransformer(cfg.embedding.model_name)

    # Upstage solar embedding ëª¨ë¸ ì´ˆê¸°í™” (4096ì°¨ì›)
    solar_model = UpstageEmbeddings(model="solar-embedding-1-large")

    # Reranker ëª¨ë¸ ì´ˆê¸°í™”
    reranker_tokenizer, reranker_model = initialize_reranker(cfg)

    # Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •
    settings = {
        "analysis": {
            "analyzer": {
                "nori": {
                    "type": "custom",
                    "tokenizer": "nori_tokenizer",
                    "decompound_mode": "mixed",
                    "filter": ["nori_posfilter"]
                }
            },
            "filter": {
                "nori_posfilter": {
                    "type": "nori_part_of_speech",
                    "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
                }
            }
        }
    }

    mappings = {
        "properties": {
            "content": {"type": "text", "analyzer": "nori"},
            "embeddings": {
                "type": "dense_vector",
                "dims": 768,
                "index": True,
                "similarity": "l2_norm"
            }
        }
    }

    # ì¸ë±ìŠ¤ ìƒì„±
    create_es_index(es, cfg.index.name, settings, mappings)

    # ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹± (solar: 4096ì°¨ì›, ES: 768ì°¨ì›)
    index_docs = []
    with open(cfg.paths.documents) as f:
        docs = [json.loads(line) for line in f]

    # solar embedding ì „ì²´ ìƒì„±
    solar_embeds = get_embeddings_in_batches(docs, solar_model, cfg.embedding.batch_size)

    for doc, solar_embed in zip(docs, solar_embeds):
        # Elasticsearchìš© ì„ë² ë”© (ì• 768ê°œë§Œ ì‚¬ìš©, ì‹¤ì œë¡œëŠ” ì°¨ì› ì¶•ì†Œ í•„ìš”)
        doc["embeddings"] = solar_embed[:768]
        index_docs.append(doc)

    # ëŒ€ëŸ‰ ë¬¸ì„œ ì¶”ê°€
    ret = bulk_add(es, cfg.index.name, index_docs)
    log.info(f'Bulk indexing completed: {ret}')

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
    test_query = "ê¸ˆì„±ì´ ë‹¤ë¥¸ í–‰ì„±ë“¤ë³´ë‹¤ ë°ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    log.info(f'Running test query: {test_query}')
    
    # ì—­ìƒ‰ì¸ì„ ì‚¬ìš©í•˜ëŠ” ê²€ìƒ‰ ì˜ˆì œ
    search_result_retrieve = sparse_retrieve(es, cfg.index.name, test_query, cfg.search.sparse.top_k)
    log.info('Sparse retrieval results:')
    for rst in search_result_retrieve['hits']['hits']:
        log.info(f'Score: {rst["_score"]:.4f}, Content: {rst["_source"]["content"][:100]}...')

    # Vector ìœ ì‚¬ë„ ì‚¬ìš©í•œ ê²€ìƒ‰ ì˜ˆì œ
    search_result_retrieve = dense_retrieve(es, solar_model, cfg.index.name, test_query, cfg.search.dense.top_k, cfg.search.dense.num_candidates)
    log.info('Dense retrieval results:')
    for rst in search_result_retrieve['hits']['hits']:
        log.info(f'Score: {rst["_score"]:.4f}, Content: {rst["_source"]["content"][:100]}...')

    # í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ìƒì„±
    # CSV íŒŒì¼ì„ Hydra outputs ë””ë ‰í† ë¦¬ì— ì €ì¥
    from hydra.core.hydra_config import HydraConfig
    hydra_output_dir = HydraConfig.get().runtime.output_dir
    output_path = os.path.join(hydra_output_dir, cfg.paths.output)
    
    log.info(f'Current working directory: {os.getcwd()}')
    log.info(f'Hydra output directory: {hydra_output_dir}')
    log.info(f'Starting evaluation with output file: {output_path}')
    eval_rag(cfg.paths.eval_data, output_path, client, cfg, es, cfg.index.name, solar_model, reranker_tokenizer, reranker_model)
    log.info('RAG evaluation process completed')


if __name__ == "__main__":
    main()

