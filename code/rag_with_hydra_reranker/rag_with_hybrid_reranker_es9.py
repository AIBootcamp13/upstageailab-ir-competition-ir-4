import os
import json
import logging
import psutil
from dotenv import load_dotenv
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from langchain_upstage import UpstageEmbeddings
import hydra
from omegaconf import DictConfig
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def log_memory_usage(log, message=""):
    """CPUì™€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¡œê·¸ì— ì¶œë ¥"""
    try:
        # CPU ë©”ëª¨ë¦¬ (RAM)
        process = psutil.Process()
        memory_info = process.memory_info()
        cpu_memory_mb = memory_info.rss / 1024 / 1024  # MB ë‹¨ìœ„

        # ì‹œìŠ¤í…œ ì „ì²´ ë©”ëª¨ë¦¬
        system_memory = psutil.virtual_memory()
        total_memory_gb = system_memory.total / 1024 / 1024 / 1024  # GB ë‹¨ìœ„
        available_memory_gb = system_memory.available / 1024 / 1024 / 1024  # GB ë‹¨ìœ„
        used_percent = system_memory.percent

        log_msg = f"{message} - CPU ë©”ëª¨ë¦¬: {cpu_memory_mb:.1f}MB, ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {used_percent:.1f}% ì‚¬ìš© ({available_memory_gb:.1f}GB/{total_memory_gb:.1f}GB ê°€ìš©)"

        # GPU ë©”ëª¨ë¦¬ (PyTorch CUDA ì‚¬ìš© ì‹œ)
        if torch.cuda.is_available():
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024 / 1024 / 1024  # GB ë‹¨ìœ„
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024 / 1024 / 1024  # GB ë‹¨ìœ„
            gpu_total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024  # GB ë‹¨ìœ„
            log_msg += f", GPU ë©”ëª¨ë¦¬: {gpu_memory_allocated:.1f}GB í• ë‹¹/{gpu_memory_reserved:.1f}GB ì˜ˆì•½/{gpu_total_memory:.1f}GB ì´ìš©ëŸ‰"
        else:
            log_msg += ", GPU: ì‚¬ìš© ë¶ˆê°€"

        log.info(log_msg)
    except Exception as e:
        log.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹¤íŒ¨: {e}")

# Sentence Transformer ëª¨ë¸ - main í•¨ìˆ˜ì—ì„œ ì´ˆê¸°í™”


# Upstage / SBERT ì„ë² ë”© ìœ í‹¸
def upstage_get_embedding(sentences, model, is_query=False):
    if is_query:
        return model.embed_query(sentences[0]) if len(sentences) == 1 else [model.embed_query(q) for q in sentences]
    else:
        return model.embed_documents(sentences)

def sbert_get_embedding(sentences, model):
    return model.encode(sentences)

# Gemini ì„ë² ë”© ìœ í‹¸ (ë‹¨ìˆœí™”ë¨)
def gemini_get_embedding(sentences, model, is_query=False):
    """
    Gemini ì„ë² ë”© ìƒì„± (ë‹¨ìˆœí™”ëœ ë²„ì „)

    Args:
        sentences: ì„ë² ë”©í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        model: GoogleGenerativeAIEmbeddings ëª¨ë¸
        is_query: ì§ˆì˜ìš© ì„ë² ë”©ì¸ì§€ ì—¬ë¶€
    """
    if is_query:
        # ì§ˆì˜ëŠ” ë³´í†µ ë‹¨ì¼ ë¬¸ì¥
        if len(sentences) == 1:
            return model.embed_query(sentences[0])
        else:
            return [model.embed_query(q) for q in sentences]
    else:
        # ë¬¸ì„œ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬ëŠ” ìƒìœ„ í•¨ìˆ˜ì—ì„œ ìˆ˜í–‰)
        return model.embed_documents(sentences)


# ë¬¸ì„œ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (passage embedding)
def get_embeddings_in_batches(docs, model, batch_size=100):
    log = logging.getLogger(__name__)
    batch_embeddings = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        contents = [doc["content"] for doc in batch]
        embeddings = upstage_get_embedding(contents, model, is_query=False)
        batch_embeddings.extend(embeddings)
        log.info(f'Processing batch {i}')
    return batch_embeddings



# ìƒˆë¡œìš´ index ìƒì„±
def create_es_index(es, index, settings, mappings, force_recreate=True):
    # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if es.indices.exists(index=index):
        if force_recreate:
            # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì„¤ì •ì„ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ ê°±ì‹ í•˜ê¸° ìœ„í•´ ì‚­ì œ
            es.indices.delete(index=index)
            # ì§€ì •ëœ ì„¤ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
            es.indices.create(index=index, settings=settings, mappings=mappings)
        # force_recreateê°€ Falseë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    else:
        # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        es.indices.create(index=index, settings=settings, mappings=mappings)


# ì§€ì •ëœ ì¸ë±ìŠ¤ ì‚­ì œ
def delete_es_index(es, index):
    es.indices.delete(index=index)


# Elasticsearch í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ ì¸ë±ì‹± ìˆ˜í–‰
def bulk_add(es, index, docs):
    """
    ëŒ€ëŸ‰ ì¸ë±ì‹±ì„ ìˆ˜í–‰ (íƒ€ì„ì•„ì›ƒ ì„¤ì • í¬í•¨)

    Args:
        es: Elasticsearch í´ë¼ì´ì–¸íŠ¸
        index: ì¸ë±ìŠ¤ ì´ë¦„
        docs: ì¸ë±ì‹±í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        (ì„±ê³µí•œ ë¬¸ì„œ ìˆ˜, ì‹¤íŒ¨í•œ ë¬¸ì„œ ìˆ˜)
    """
    # ëŒ€ëŸ‰ ì¸ë±ì‹± ì‘ì—…ì„ ì¤€ë¹„
    actions = [
        {
            '_index': index,
            '_source': doc
        }
        for doc in docs
    ]

    return helpers.bulk(
        es,
        actions,
        request_timeout=120,  # 120ì´ˆ íƒ€ì„ì•„ì›ƒ
        max_retries=3,        # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„
        initial_backoff=2,    # 2ì´ˆ ì´ˆê¸° ë°±ì˜¤í”„
        max_backoff=60        # ìµœëŒ€ 60ì´ˆ ë°±ì˜¤í”„
    )


# ì—­ìƒ‰ì¸ì„ ì´ìš©í•œ ê²€ìƒ‰
def sparse_retrieve(es, index_name, query_str, size):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    return es.search(index=index_name, query=query, size=size, sort="_score")


# Vector ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ê²€ìƒ‰ (backendë³„)
def dense_retrieve_upstage(es, model, index_name, query_str, size, num_candidates=100):
    log = logging.getLogger(__name__)
    # ì¿¼ë¦¬ ì„ë² ë”© (4096ì°¨ì›)
    query_embedding = upstage_get_embedding([query_str], model, is_query=True)
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()

    # num_candidates ë³´ì • (k <= num_candidates ìœ ì§€)
    orig_num_cand = num_candidates
    if num_candidates < size:
        num_candidates = size
        log.warning(f"[Upstage] num_candidates({orig_num_cand}) < k/size({size}) â†’ num_candidates={num_candidates}ë¡œ ë³´ì •")

    knn = {
        "field": "embeddings_upstage",
        "query_vector": query_embedding,
        "k": size,
        "num_candidates": num_candidates
    }
    # top-level sizeë¡œ ìµœì¢… ë°˜í™˜ ê°œìˆ˜ ì§€ì •
    return es.search(index=index_name, knn=knn, size=size)

def dense_retrieve_sbert(es, model, index_name, query_str, size, num_candidates=100):
    log = logging.getLogger(__name__)
    query_embedding = sbert_get_embedding([query_str], model)[0]
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()

    # num_candidates ë³´ì •
    orig_num_cand = num_candidates
    if num_candidates < size:
        num_candidates = size
        log.warning(f"[SBERT] num_candidates({orig_num_cand}) < k/size({size}) â†’ num_candidates={num_candidates}ë¡œ ë³´ì •")

    knn = {
        "field": "embeddings_sbert",
        "query_vector": query_embedding,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=index_name, knn=knn, size=size)

# HyDE ìºì‹œ (ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ)
_hyde_cache = {}

# HyDE ê¸°ë²•: ê°€ìƒ ë¬¸ì„œ ìƒì„± í•¨ìˆ˜ (ë²”ìš©)
def generate_hypothetical_document(query, client, cfg):
    """ì§ˆì˜ì— ëŒ€í•´ LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ìƒì˜ ë‹µë³€ ë¬¸ì„œë¥¼ ìƒì„± (OpenAI/Gemini í˜¸í™˜)"""
    log = logging.getLogger(__name__)

    # ìºì‹œ í™•ì¸
    cache_key = f"{query}_{getattr(cfg.prompts, 'hyde', '')}"
    if cache_key in _hyde_cache:
        log.debug(f"HyDE ìºì‹œì—ì„œ ë¬¸ì„œ ë°˜í™˜: {query[:50]}...")
        return _hyde_cache[cache_key]
    try:
        # í†µí•©ëœ hyde í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        hyde_prompt = getattr(cfg.prompts, 'hyde', 'ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ì„¤ëª… ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.')

        # í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜ ì‚¬ìš©
        messages = [
            {"role": "system", "content": hyde_prompt},
            {"role": "user", "content": query}
        ]

        result = call_llm_unified(
            client=client,
            messages=messages,
            cfg=cfg
        )

        # result íƒ€ì…ì— ë”°ë¼ ì ì ˆíˆ ì²˜ë¦¬
        if isinstance(result, dict):
            hypothetical_doc = result["choices"][0]["message"]["content"]
        else:
            hypothetical_doc = result.choices[0].message.content
        log.debug(f"Generated hypothetical document for query: {query[:50]}...")

        # ì„¤ì •ì— ë”°ë¼ ìƒì„±ëœ ë¬¸ì„œ ì¶œë ¥
        if getattr(cfg.logging, 'show_hyde_generated_document', False):
            log.info(f"Dense Retrieval HyDE ìƒì„± ë¬¸ì„œ (ì§ˆì˜: {query[:30]}...):\n{hypothetical_doc}")

        # ìºì‹œì— ì €ì¥
        _hyde_cache[cache_key] = hypothetical_doc

        return hypothetical_doc

    except Exception as e:
        log.warning(f"Failed to generate hypothetical document: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì§ˆì˜ ë°˜í™˜
        return query

# HyDE ê¸°ë²•ì„ í™œìš©í•œ Upstage Dense Retrieve
def dense_retrieve_upstage_hyde(es, model, index_name, query_str, size, num_candidates, client, cfg):
    """HyDE ê¸°ë²•: ì§ˆì˜ -> ê°€ìƒë¬¸ì„œ ìƒì„± -> ì„ë² ë”© -> ê²€ìƒ‰"""
    # 1ë‹¨ê³„: ê°€ìƒ ë¬¸ì„œ ìƒì„±
    hypothetical_doc = generate_hypothetical_document(query_str, client, cfg)

    # 2ë‹¨ê³„: ê°€ìƒ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ê²€ìƒ‰ (ê¸°ì¡´ dense_retrieve_upstageì™€ ë™ì¼)
    log = logging.getLogger(__name__)
    query_embedding = upstage_get_embedding([hypothetical_doc], model, is_query=True)
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()

    # num_candidates ë³´ì •
    orig_num_cand = num_candidates
    if num_candidates < size:
        num_candidates = size
        log.warning(f"[Upstage-HyDE] num_candidates({orig_num_cand}) < k/size({size}) â†’ num_candidates={num_candidates}ë¡œ ë³´ì •")

    knn = {
        "field": "embeddings_upstage",
        "query_vector": query_embedding,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=index_name, knn=knn, size=size)

# Gemini Vector ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ê²€ìƒ‰
def dense_retrieve_gemini(es, model, index_name, query_str, size, num_candidates=100):
    """Gemini ì„ë² ë”©ì„ ì‚¬ìš©í•œ dense retrieve"""
    # ì¿¼ë¦¬ ì„ë² ë”© (3072ì°¨ì›)
    log = logging.getLogger(__name__)
    query_embedding = gemini_get_embedding([query_str], model, is_query=True)
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()

    # num_candidates ë³´ì •
    orig_num_cand = num_candidates
    if num_candidates < size:
        num_candidates = size
        log.warning(f"[Gemini] num_candidates({orig_num_cand}) < k/size({size}) â†’ num_candidates={num_candidates}ë¡œ ë³´ì •")

    knn = {
        "field": "embeddings_gemini",
        "query_vector": query_embedding,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=index_name, knn=knn, size=size)

# HyDE ê¸°ë²•ì„ í™œìš©í•œ Gemini Dense Retrieve
def dense_retrieve_gemini_hyde(es, model, index_name, query_str, size, num_candidates, client, cfg):
    """HyDE ê¸°ë²•: ì§ˆì˜ -> ê°€ìƒë¬¸ì„œ ìƒì„± -> Gemini ì„ë² ë”© -> ê²€ìƒ‰"""
    # 1ë‹¨ê³„: ê°€ìƒ ë¬¸ì„œ ìƒì„±
    hypothetical_doc = generate_hypothetical_document(query_str, client, cfg)

    # 2ë‹¨ê³„: ê°€ìƒ ë¬¸ì„œë¥¼ Gemini ì„ë² ë”©í•˜ì—¬ ê²€ìƒ‰
    log = logging.getLogger(__name__)
    query_embedding = gemini_get_embedding([hypothetical_doc], model, is_query=True)
    if hasattr(query_embedding, 'tolist'):
        query_embedding = query_embedding.tolist()

    # num_candidates ë³´ì •
    orig_num_cand = num_candidates
    if num_candidates < size:
        num_candidates = size
        log.warning(f"[Gemini-HyDE] num_candidates({orig_num_cand}) < k/size({size}) â†’ num_candidates={num_candidates}ë¡œ ë³´ì •")

    knn = {
        "field": "embeddings_gemini",
        "query_vector": query_embedding,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=index_name, knn=knn, size=size)

def retrieve_all(es, index_name):
    """ëª¨ë“  ë¬¸ì„œë¥¼ ì¡°íšŒí•˜ì—¬ ë¦¬ë­í‚¹ í›„ë³´ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    scoreëŠ” 0.0ìœ¼ë¡œ ì„¤ì •í•œë‹¤.
    """
    documents = []
    # helpers.scanì€ ì „ì²´ ë¬¸ì„œë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìˆœíšŒ
    for hit in helpers.scan(es, index=index_name, query={"query": {"match_all": {}}}):
        src = hit.get("_source", {})
        docid = src.get("docid", hit.get("_id"))
        content = src.get("content", "")
        documents.append({
            "content": content,
            "docid": docid,
            "score": 0.0
        })
    return documents

# ê³µì‹ ì‚¬ìš©ë²• ê¸°ë°˜ Qwen3-Reranker-8B ì´ˆê¸°í™” (CausalLM + yes/no)
def initialize_reranker(cfg):
    log = logging.getLogger(__name__)
    if not cfg.reranker.use_reranker:
        return None, None, None

    try:
        log.info(f"Loading reranker model (CausalLM): {cfg.reranker.model_name}")

        kwargs = {}
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if device == 'cuda':
            kwargs.update({
                'dtype': torch.float16,
            })

        tokenizer = AutoTokenizer.from_pretrained(cfg.reranker.model_name, padding_side='left')

        # pad í† í° ë³´ì¥ (ì¼ë¶€ í† í¬ë‚˜ì´ì €ëŠ” pad_tokenì´ ì—†ì„ ìˆ˜ ìˆìŒ)
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
            elif tokenizer.sep_token is not None:
                tokenizer.pad_token = tokenizer.sep_token
            else:
                tokenizer.add_special_tokens({"pad_token": "[PAD]"})

        model = AutoModelForCausalLM.from_pretrained(cfg.reranker.model_name, **kwargs).to(device).eval()

        # yes/no í† í° id
        token_false_id = tokenizer.convert_tokens_to_ids("no")
        token_true_id = tokenizer.convert_tokens_to_ids("yes")
        max_length = 8192

        # ê³µì‹ í”„ë¦¬í”½ìŠ¤/ì„œí”½ìŠ¤ í…œí”Œë¦¿
        prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)
        suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)

        aux = {
            'device': device,
            'token_true_id': token_true_id,
            'token_false_id': token_false_id,
            'max_length': max_length,
            'prefix_tokens': prefix_tokens,
            'suffix_tokens': suffix_tokens,
        }
        log.info("Reranker(CausalLM) initialized")
        return tokenizer, model, aux
    except Exception as e:
        log.error(f"Failed to load reranker model: {e}")
        return None, None, None


def _format_instruction(instruction, query, doc):
    if instruction is None or len(str(instruction).strip()) == 0:
        instruction = 'Given a web search query, retrieve relevant passages that answer the query'
    return "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
        instruction=instruction, query=query, doc=doc
    )


# ê³µì‹ ì‚¬ìš©ë²• ê¸°ë°˜ reranking (ë°°ì¹˜ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ëŠ¥ ì¶”ê°€)
def rerank_documents(query, documents, reranker_tokenizer, reranker_model, reranker_aux, cfg, client=None):
    log = logging.getLogger(__name__)

    if reranker_tokenizer is None or reranker_model is None or reranker_aux is None:
        log.warning("Reranker not initialized, returning original order")
        return documents[:cfg.reranker.top_k]

    try:
        instruction = getattr(cfg.reranker, 'instruction', None)
        # configì—ì„œ batch_sizeë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’(ì˜ˆ: 2)ìœ¼ë¡œ ì„¤ì •
        batch_size = getattr(cfg.reranker, 'batch_size', 2)

        # HyDE ê¸°ë²• ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_hyde = getattr(cfg.reranker, 'use_hyde', False)
        rerank_query = query

        if use_hyde and client is not None:
            log.info("Reranker HyDE ê¸°ë²• í™œì„±í™” - ê°€ìƒ ë¬¸ì„œ ìƒì„± ì¤‘...")
            rerank_query = generate_hypothetical_document(query, client, cfg)
        elif use_hyde and client is None:
            log.warning("Reranker HyDE í™œì„±í™”ë˜ì—ˆìœ¼ë‚˜ clientê°€ ì—†ì–´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")

        log.info(f"Reranking with {'HyDE query' if use_hyde and client else 'original query'}")

        # ì‹¤ì œ ì¿¼ë¦¬ (ì›ë³¸ ë˜ëŠ” HyDE ìƒì„±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬ë­í‚¹ ìˆ˜í–‰
        actual_query = rerank_query
        
        all_scores = []
        device = reranker_aux['device']
        
        # ë¬¸ì„œë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(documents), batch_size):
            log.info(f"Reranking batch {i // batch_size + 1}...")
            batch_docs = documents[i:i + batch_size]
            
            # ì…ë ¥ ë¬¸ìì—´ ìƒì„± (HyDE ì¿¼ë¦¬ ë˜ëŠ” ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)
            pairs = [_format_instruction(instruction, actual_query, doc["content"]) for doc in batch_docs]

            # í† í¬ë‚˜ì´ì¦ˆ
            max_length = reranker_aux['max_length']
            prefix_tokens = reranker_aux['prefix_tokens']
            suffix_tokens = reranker_aux['suffix_tokens']

            inputs = reranker_tokenizer(
                pairs,
                padding=False,
                truncation='longest_first',
                return_attention_mask=False,
                max_length=max_length - len(prefix_tokens) - len(suffix_tokens)
            )
            for j, ids in enumerate(inputs['input_ids']):
                inputs['input_ids'][j] = prefix_tokens + ids + suffix_tokens

            inputs = reranker_tokenizer.pad(inputs, padding=True, return_tensors="pt", max_length=max_length)
            
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            for key in inputs:
                inputs[key] = inputs[key].to(device)

            # ë°°ì¹˜ ì¶”ë¡ 
            with torch.no_grad():
                batch_scores_logits = reranker_model(**inputs).logits[:, -1, :]
                true_vector = batch_scores_logits[:, reranker_aux['token_true_id']]
                false_vector = batch_scores_logits[:, reranker_aux['token_false_id']]
                stacked_scores = torch.stack([false_vector, true_vector], dim=1)
                log_softmax_scores = torch.nn.functional.log_softmax(stacked_scores, dim=1)
                scores = log_softmax_scores[:, 1].exp().tolist()
            
            all_scores.extend(scores)

            # === ë©”ëª¨ë¦¬ ê´€ë¦¬ ì½”ë“œ ì¶”ê°€ ===
            # í˜„ì¬ ë°°ì¹˜ì—ì„œ ì‚¬ìš©í•œ í…ì„œë“¤ì„ GPU ë©”ëª¨ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ
            del inputs, batch_scores_logits, true_vector, false_vector, stacked_scores, log_softmax_scores
            # PyTorchê°€ ìºì‹±í•˜ê³  ìˆëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ë©”ëª¨ë¦¬ë¥¼ GPUì—ì„œ í•´ì œ
            torch.cuda.empty_cache()
            # =========================

        # ì „ì²´ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        doc_score_pairs = list(zip(documents, all_scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        reranked_docs = [doc for doc, _ in doc_score_pairs[:cfg.reranker.top_k]]

        log.info(f"Reranked {len(documents)} documents to top {len(reranked_docs)} using batch size {batch_size} with memory clearing")
        return reranked_docs

    except Exception as e:
        log.error(f"Error during reranking: {e}", exc_info=True) # ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
        return documents[:cfg.reranker.top_k]


# Elasticsearch ì„¤ì •ì€ main í•¨ìˆ˜ì—ì„œ ì´ˆê¸°í™”

# Elasticsearch ì„¤ì •ë“¤ì€ main í•¨ìˆ˜ì—ì„œ ì •ì˜

# ì´ˆê¸°í™” ì½”ë“œë“¤ì€ main í•¨ìˆ˜ë¡œ ì´ë™

# RAGë¥¼ êµ¬í˜„í•˜ëŠ” ì½”ë“œ
from openai import OpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings  # embeddingë§Œ LangChain ì‚¬ìš©
from google import genai
from google.genai import types
import traceback
import time

# OpenAI ë° Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„± í•¨ìˆ˜
def create_llm_client(cfg):
    """ì„¤ì •ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± (OpenAI ë˜ëŠ” Gemini)"""
    log = logging.getLogger(__name__)
    model_name = cfg.llm.model

    # Gemini ëª¨ë¸ì¸ì§€ í™•ì¸
    if "gemini" in model_name.lower():
        # GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEY ì§€ì›
        google_api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError("Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ GEMINI_API_KEY ë˜ëŠ” GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        log.info(f"Gemini LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±: {model_name}")
        return genai.Client(api_key=google_api_key)
    else:
        # OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ (solar-pro2 ë“±)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OpenAI í˜¸í™˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        log.info(f"OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„±: {model_name}")
        openai_base_url = os.getenv("OPENAI_BASE_URL")
        if openai_base_url:
            return OpenAI(base_url=openai_base_url)
        else:
            return OpenAI()

def apply_llm_delay(cfg):
    """ì„¤ì •ëœ ì‹œê°„ë§Œí¼ ëŒ€ê¸° (rate limit íšŒí”¼)"""
    delay_seconds = getattr(cfg.llm, 'delay_seconds', 0)
    if delay_seconds > 0:
        log = logging.getLogger(__name__)
        log.debug(f"LLM í˜¸ì¶œ ëŒ€ê¸°: {delay_seconds}ì´ˆ")
        time.sleep(delay_seconds)

def call_llm_unified(client, messages, cfg, tools=None, tool_choice=None):
    """OpenAI/Gemini í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜"""
    log = logging.getLogger(__name__)
    model_name = cfg.llm.model

    # rate limit íšŒí”¼ë¥¼ ìœ„í•œ ëŒ€ê¸°
    apply_llm_delay(cfg)

    # Gemini í´ë¼ì´ì–¸íŠ¸ì¸ì§€ í™•ì¸ (ë” ì •í™•í•œ íƒ€ì… ì²´í¬)
    if isinstance(client, genai.Client):  # genai.Client
        # ì¬ì‹œë„ ì„¤ì •ê°’ ë¡œë“œ (ê¸°ë³¸ê°’: ìµœëŒ€ 5íšŒ, 30ì´ˆ ëŒ€ê¸°)
        retry_max = int(getattr(cfg.llm, 'retry_max', 5) or 5)
        retry_delay = int(getattr(cfg.llm, 'retry_delay_seconds', 30) or 30)

        # OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ Gemini types.Content í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        contents = []
        for msg in messages:
            role = "user" if msg["role"] in ["user", "system"] else "model"
            # system ë©”ì‹œì§€ëŠ” userë¡œ í¬í•¨ (GeminiëŠ” system role ì—†ìŒ)
            content_text = msg["content"]
            if msg["role"] == "system":
                content_text = f"System: {content_text}"

            contents.append(types.Content(
                role=role,
                parts=[types.Part.from_text(text=content_text)]
            ))

        # Gemini tool calling ì§€ì›
        gemini_tools = None
        if tools:
            gemini_tools = []
            for tool in tools:
                if tool["type"] == "function":
                    func_def = tool["function"]
                    # Gemini FunctionDeclaration ìƒì„±
                    gemini_func = types.FunctionDeclaration(
                        name=func_def["name"],
                        description=func_def["description"],
                        parameters=func_def["parameters"]
                    )
                    gemini_tools.append(types.Tool(function_declarations=[gemini_func]))

        config = types.GenerateContentConfig(
            temperature=cfg.llm.temperature,
            tools=gemini_tools if gemini_tools else None,
        )

        # 2xxê°€ ì•„ë‹Œ ì‘ë‹µ/ì˜ˆì™¸ ë°œìƒ ì‹œ ì¬ì‹œë„
        last_exc = None
        response = None
        for attempt in range(1, retry_max + 1):
            try:
                response = client.models.generate_content(
                    model=model_name,
                    contents=contents,
                    config=config,
                )
                # google-genai í´ë¼ì´ì–¸íŠ¸ëŠ” 2xx ì™¸ì—ëŠ” ì˜ˆì™¸ë¥¼ ë˜ì§€ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ
                # ì„±ê³µì ìœ¼ë¡œ ê°ì²´ê°€ ë°˜í™˜ë˜ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰
                break
            except Exception as e:
                # ìƒíƒœ ì½”ë“œ ì¶”ì¶œ ì‹œë„
                status_code = None
                try:
                    status_code = getattr(e, 'status_code', None)
                except Exception:
                    pass
                try:
                    if status_code is None and hasattr(e, 'response') and getattr(e, 'response') is not None:
                        status_code = getattr(e.response, 'status_code', None)
                except Exception:
                    pass

                # ì˜ˆì™¸ ë©”ì‹œì§€ì—ì„œ 503 ë“± í‚¤ì›Œë“œ ì¶”ì¶œ (fallback)
                status_hint = None
                msg_text = str(e)
                if 'HTTP/1.1' in msg_text or 'Service Unavailable' in msg_text or '503' in msg_text:
                    status_hint = 'non-2xx (likely 503)'

                # ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨: 2xxê°€ ì•„ë‹ˆê±°ë‚˜ ìƒíƒœ ì½”ë“œë¥¼ ì•Œ ìˆ˜ ì—†ì–´ë„ ì˜ˆì™¸ê°€ ë°œìƒí•œ ê²½ìš° ì¬ì‹œë„
                should_retry = True
                if status_code is not None:
                    should_retry = not (200 <= int(status_code) < 300)

                last_exc = e
                if attempt < retry_max and should_retry:
                    sc_str = f"status={status_code}" if status_code is not None else (f"hint={status_hint}" if status_hint else "status=unknown")
                    log.warning(f"Gemini í˜¸ì¶œ ì‹¤íŒ¨(ì‹œë„ {attempt}/{retry_max}, {sc_str}). {retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    time.sleep(retry_delay)
                    continue
                # ë§ˆì§€ë§‰ ì‹œë„ì´ê±°ë‚˜, ì¬ì‹œë„ ëŒ€ìƒì´ ì•„ë‹ˆë©´ ì˜ˆì™¸ ì¬ì „íŒŒ
                raise

        # OpenAI í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë³€í™˜
        result = {
            "choices": [{
                "message": {
                    "content": None,
                    "role": "assistant",
                    "tool_calls": None
                }
            }]
        }

        # ì‘ë‹µì—ì„œ function call ë˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                tool_calls = []
                text_parts = []

                for part in candidate.content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        # Function call ì²˜ë¦¬
                        func_call = part.function_call
                        tool_calls.append({
                            "id": f"call_{hash(func_call.name)}",
                            "type": "function",
                            "function": {
                                "name": func_call.name,
                                "arguments": json.dumps(dict(func_call.args))
                            }
                        })
                    elif hasattr(part, 'text') and part.text:
                        # í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
                        text_parts.append(part.text)

                if tool_calls:
                    result["choices"][0]["message"]["tool_calls"] = tool_calls
                if text_parts:
                    result["choices"][0]["message"]["content"] = "".join(text_parts)

        return result
    else:  # OpenAI í´ë¼ì´ì–¸íŠ¸
        params = {
            "model": model_name,
            "messages": messages,
            "temperature": cfg.llm.temperature,
            "seed": cfg.llm.seed,
            "timeout": cfg.llm.timeout
        }

        # reasoning_effort ì§€ì› (o3 ê³„ì—´ ë“±)
        if hasattr(cfg.llm, 'reasoning_effort') and getattr(cfg.llm, 'reasoning_effort'):
            params["reasoning_effort"] = cfg.llm.reasoning_effort

        if tools:
            params["tools"] = tools
        if tool_choice:
            params["tool_choice"] = tool_choice

        return client.chat.completions.create(**params)

# í”„ë¡¬í”„íŠ¸ë“¤ì€ config.yamlì—ì„œ ê´€ë¦¬

# Function callingì— ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜
def get_tools(cfg):
    return [
        {
            "type": "function",
            "function": {
                "name": "search",
                "description": "search relevant documents",
                "parameters": {
                    "properties": {
                        "standalone_query": {
                            "type": "string",
                            "description": cfg.prompts.standalone_query_description
                        }
                    },
                    "required": ["standalone_query"],
                    "type": "object"
                }
            }
        },
    ]


# LLMê³¼ ê²€ìƒ‰ì—”ì§„ì„ í™œìš©í•œ RAG êµ¬í˜„
def answer_question(messages, client, cfg, es, index_name, dense_ctx=None, reranker_tokenizer=None, reranker_model=None, reranker_aux=None):
    # í•¨ìˆ˜ ì¶œë ¥ ì´ˆê¸°í™”
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    # ë¡œê±° ì´ˆê¸°í™”
    log = logging.getLogger(__name__)

    # ì§ˆì˜ ë¶„ì„ ë° ê²€ìƒ‰ ì´ì™¸ì˜ ì§ˆì˜ ëŒ€ì‘ì„ ìœ„í•œ LLM í™œìš©
    msg = [{"role": "system", "content": cfg.prompts.function_calling}] + messages
    try:
        result = call_llm_unified(
            client=client,
            messages=msg,
            cfg=cfg,
            tools=get_tools(cfg),
            #tool_choice={"type": "function", "function": {"name": "search"}}
        )
    except Exception:
        traceback.print_exc()
        return response

    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ê²€ìƒ‰ í˜¸ì¶œí›„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    # resultê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ ê°ì²´ì¸ì§€ í™•ì¸í•˜ì—¬ ì²˜ë¦¬
    if isinstance(result, dict):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (Gemini)
        message = result["choices"][0]["message"]
        tool_calls = message.get("tool_calls")
    else:
        # ê°ì²´ í˜•íƒœ (OpenAI)
        message = result.choices[0].message
        tool_calls = getattr(message, 'tool_calls', None)

    if tool_calls:
        tool_call = tool_calls[0]
        if isinstance(tool_call, dict):
            function_args = json.loads(tool_call["function"]["arguments"])
        else:
            function_args = json.loads(tool_call.function.arguments)
        standalone_query = function_args.get("standalone_query")

        # Standalone query ë¡œê¹…
        log.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: '{standalone_query}'")

        # ì„¤ì • í† ê¸€ì— ë”°ë¥¸ ê²€ìƒ‰ ë™ì‘ ë¶„ê¸°
        response["standalone_query"] = standalone_query

        sparse_enabled = getattr(cfg.retrieve.sparse, 'enabled', True)
        upstage_enabled = getattr(cfg.retrieve.dense_upstage, 'enabled', False)
        sbert_enabled = getattr(cfg.retrieve.dense_sbert, 'enabled', False)
        upstage_hyde_enabled = getattr(cfg.retrieve.dense_upstage_hyde, 'enabled', False)
        gemini_enabled = getattr(cfg.retrieve.dense_gemini, 'enabled', False)
        gemini_hyde_enabled = getattr(cfg.retrieve.dense_gemini_hyde, 'enabled', False)

        documents = []
        if not sparse_enabled and not upstage_enabled and not sbert_enabled and not upstage_hyde_enabled and not gemini_enabled and not gemini_hyde_enabled:
            # ë¦¬íŠ¸ë¦¬ë¸Œ ë¹„í™œì„±í™”: ì „ì²´ ë¬¸ì„œë¥¼ ë¦¬ë­í‚¹ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
            documents = retrieve_all(es, index_name)
        else:
            docids = set()
            # ê° retrieve ë°©ì‹ë³„ ë¬¸ì„œ ìˆ˜ì§‘ í˜„í™© ì¶”ì ì„ ìœ„í•œ ì¹´ìš´í„°
            sparse_count = 0
            upstage_count = 0
            sbert_count = 0
            hyde_count = 0
            gemini_count = 0
            gemini_hyde_count = 0

            # ê° retrieve ë°©ì‹ë³„ DocID ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
            sparse_docids = []
            upstage_docids = []
            sbert_docids = []
            hyde_docids = []
            gemini_docids = []
            gemini_hyde_docids = []
            if sparse_enabled:
                sparse_result = sparse_retrieve(es, index_name, standalone_query, cfg.retrieve.sparse.top_k)
                sparse_retrieved = len(sparse_result['hits']['hits'])
                for rst in sparse_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        sparse_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            sparse_count += 1
                docid_info = f" - DocIDs: {sparse_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Sparse retrieve: {sparse_retrieved}ê°œ ê²€ìƒ‰, {sparse_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {sparse_retrieved - sparse_count}ê°œ){docid_info}")
            # í•˜ì´ë¸Œë¦¬ë“œ dense: upstage â†’ sbert ìˆœì„œë¡œ ë§ë¶™ì„
            if upstage_enabled and dense_ctx and dense_ctx.get('upstage'):
                du = dense_ctx['upstage']
                dense_result = dense_retrieve_upstage(
                    es, du.get('model'), index_name, standalone_query,
                    cfg.retrieve.dense_upstage.top_k, cfg.retrieve.dense_upstage.num_candidates
                )
                upstage_retrieved = len(dense_result['hits']['hits'])
                for rst in dense_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        upstage_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            upstage_count += 1
                docid_info = f" - DocIDs: {upstage_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Dense Upstage retrieve: {upstage_retrieved}ê°œ ê²€ìƒ‰, {upstage_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {upstage_retrieved - upstage_count}ê°œ){docid_info}")
            if sbert_enabled and dense_ctx and dense_ctx.get('sbert'):
                ds = dense_ctx['sbert']
                dense_result = dense_retrieve_sbert(
                    es, ds.get('model'), index_name, standalone_query,
                    cfg.retrieve.dense_sbert.top_k, cfg.retrieve.dense_sbert.num_candidates
                )
                sbert_retrieved = len(dense_result['hits']['hits'])
                for rst in dense_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        sbert_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            sbert_count += 1
                docid_info = f" - DocIDs: {sbert_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Dense SBERT retrieve: {sbert_retrieved}ê°œ ê²€ìƒ‰, {sbert_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {sbert_retrieved - sbert_count}ê°œ){docid_info}")
            # HyDE ê¸°ë²•ì„ í™œìš©í•œ Upstage Dense Retrieve
            if upstage_hyde_enabled and dense_ctx and dense_ctx.get('upstage_hyde'):
                duh = dense_ctx['upstage_hyde']
                dense_result = dense_retrieve_upstage_hyde(
                    es, duh.get('model'), index_name, standalone_query,
                    cfg.retrieve.dense_upstage_hyde.top_k, cfg.retrieve.dense_upstage_hyde.num_candidates,
                    client, cfg
                )
                hyde_retrieved = len(dense_result['hits']['hits'])
                for rst in dense_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        hyde_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            hyde_count += 1
                docid_info = f" - DocIDs: {hyde_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Dense Upstage HyDE retrieve: {hyde_retrieved}ê°œ ê²€ìƒ‰, {hyde_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {hyde_retrieved - hyde_count}ê°œ){docid_info}")

            # Gemini Dense Retrieve
            if gemini_enabled and dense_ctx and dense_ctx.get('gemini'):
                dg = dense_ctx['gemini']
                dense_result = dense_retrieve_gemini(
                    es, dg.get('model'), index_name, standalone_query,
                    cfg.retrieve.dense_gemini.top_k, cfg.retrieve.dense_gemini.num_candidates
                )
                gemini_retrieved = len(dense_result['hits']['hits'])
                for rst in dense_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        gemini_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            gemini_count += 1
                docid_info = f" - DocIDs: {gemini_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Dense Gemini retrieve: {gemini_retrieved}ê°œ ê²€ìƒ‰, {gemini_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {gemini_retrieved - gemini_count}ê°œ){docid_info}")

            # HyDE ê¸°ë²•ì„ í™œìš©í•œ Gemini Dense Retrieve
            if gemini_hyde_enabled and dense_ctx and dense_ctx.get('gemini_hyde'):
                dgh = dense_ctx['gemini_hyde']
                dense_result = dense_retrieve_gemini_hyde(
                    es, dgh.get('model'), index_name, standalone_query,
                    cfg.retrieve.dense_gemini_hyde.top_k, cfg.retrieve.dense_gemini_hyde.num_candidates,
                    client, cfg
                )
                gemini_hyde_retrieved = len(dense_result['hits']['hits'])
                for rst in dense_result['hits']['hits']:
                    src = rst.get("_source", {})
                    docid = src.get("docid")
                    if docid:
                        gemini_hyde_docids.append(docid)  # ê°€ì ¸ì˜¨ ëª¨ë“  docid ìˆ˜ì§‘
                        if docid not in docids:
                            docids.add(docid)
                            documents.append({
                                "content": src.get("content", ""),
                                "docid": docid,
                                "score": rst.get("_score", 0.0)
                            })
                            gemini_hyde_count += 1
                docid_info = f" - DocIDs: {gemini_hyde_docids}" if getattr(cfg.logging, 'show_retrieved_docids', False) else ""
                log.info(f"Dense Gemini HyDE retrieve: {gemini_hyde_retrieved}ê°œ ê²€ìƒ‰, {gemini_hyde_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {gemini_hyde_retrieved - gemini_hyde_count}ê°œ){docid_info}")

            # ì „ì²´ retrieve ìš”ì•½ ë¡œê·¸ ì¶œë ¥
            active_methods = []
            if sparse_enabled and sparse_count > 0:
                active_methods.append(f"Sparse({sparse_count})")
            if upstage_enabled and upstage_count > 0:
                active_methods.append(f"Upstage({upstage_count})")
            if sbert_enabled and sbert_count > 0:
                active_methods.append(f"SBERT({sbert_count})")
            if upstage_hyde_enabled and hyde_count > 0:
                active_methods.append(f"UpstageHyDE({hyde_count})")
            if gemini_enabled and gemini_count > 0:
                active_methods.append(f"Gemini({gemini_count})")
            if gemini_hyde_enabled and gemini_hyde_count > 0:
                active_methods.append(f"GeminiHyDE({gemini_hyde_count})")

            total_docs = len(documents)
            summary = " + ".join(active_methods) if active_methods else "ì—†ìŒ"
            log.info(f"ğŸ“Š Retrieve ìš”ì•½: {summary} = ì´ {total_docs}ê°œ ë¬¸ì„œ")

        # Rerankerê°€ í™œì„±í™”ëœ ê²½ìš° reranking ìˆ˜í–‰
        if cfg.reranker.use_reranker and reranker_tokenizer is not None and reranker_model is not None:
            reranked_documents = rerank_documents(standalone_query, documents, reranker_tokenizer, reranker_model, reranker_aux, cfg, client)
        else:
            # Rerankerê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ìƒìœ„ top_kê°œë§Œ ì„ íƒ
            reranked_documents = documents[:cfg.reranker.top_k]
        
        # ìµœì¢… ê²°ê³¼ë¥¼ responseì— ì €ì¥
        retrieved_context = []
        for doc in reranked_documents:
            retrieved_context.append(doc["content"])
            response["topk"].append(doc["docid"])
            response["references"].append({"score": doc["score"], "content": doc["content"]})

        if cfg.qa.use_final_answer:
            # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³„ë„ QA ìˆ˜í–‰
            content = json.dumps(retrieved_context)
            messages.append({"role": "assistant", "content": content})
            msg = [{"role": "system", "content": cfg.prompts.qa}] + messages
            try:
                qaresult = call_llm_unified(
                    client=client,
                    messages=msg,
                    cfg=cfg
                )
            except Exception:
                traceback.print_exc()
                return response
            # qaresultë„ ë”•ì…”ë„ˆë¦¬/ê°ì²´ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
            if isinstance(qaresult, dict):
                response["answer"] = qaresult["choices"][0]["message"]["content"]
            else:
                response["answer"] = qaresult.choices[0].message.content
        else:
            # í˜„ì¬ ë°©ì‹: ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
            if isinstance(result, dict):
                response["answer"] = result["choices"][0]["message"]["content"]
            else:
                response["answer"] = result.choices[0].message.content

    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ë°”ë¡œ ë‹µë³€ ìƒì„±
    else:
        if isinstance(result, dict):
            response["answer"] = result["choices"][0]["message"]["content"]
        else:
            response["answer"] = result.choices[0].message.content

    return response


# í‰ê°€ë¥¼ ìœ„í•œ íŒŒì¼ì„ ì½ì–´ì„œ ê° í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ì¶”ì¶œí›„ íŒŒì¼ì— ì €ì¥
def eval_rag(eval_filename, output_filename, client, cfg, es, index_name, dense_ctx=None):
    log = logging.getLogger(__name__)

    # ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    log_memory_usage(log, "ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ ì „")

    # ë¦¬ë­í‚¹ ëª¨ë¸ì„ í‰ê°€ ì‹œì‘ ì§ì „ì— ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
    reranker_tokenizer, reranker_model, reranker_aux = initialize_reranker(cfg)

    # ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    log_memory_usage(log, "ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ í›„")

    general_questions = []  # ì¼ë°˜ì§ˆë¬¸ eval_id, answer ì €ì¥ ë¦¬ìŠ¤íŠ¸
    general_eval_ids = []   # eval_idë§Œ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    with open(eval_filename) as f, open(output_filename, "w") as of:
        idx = 0
        for line in f:
            j = json.loads(line)
            log.info(f'ğŸš©Test {idx + 1} - Question: {j["msg"]}')
            response = answer_question(j["msg"], client, cfg, es, index_name, dense_ctx, reranker_tokenizer, reranker_model, reranker_aux)
            log.info(f'Answer: {response["answer"]}')
            log.info(f'Retrieved {"ğŸ‘†ì¼ë°˜ì§ˆë¬¸ğŸ‘†" if len(response["topk"]) == 0 else len(response["topk"])} documents: {response["topk"]}')
            log.debug(f'References: {len(response["references"])} items')

            # ì¼ë°˜ì§ˆë¬¸ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            if len(response["topk"]) == 0:
                general_questions.append({"eval_id": j["eval_id"], "answer": response["answer"]})
                general_eval_ids.append(j["eval_id"])

            # ëŒ€íšŒ score ê³„ì‚°ì€ topk ì •ë³´ë¥¼ ì‚¬ìš©, answer ì •ë³´ëŠ” LLMì„ í†µí•œ ìë™í‰ê°€ì‹œ í™œìš©
            output = {"eval_id": j["eval_id"], "standalone_query": response["standalone_query"], "topk": response["topk"], "answer": response["answer"], "references": response["references"]}
            of.write(f'{json.dumps(output, ensure_ascii=False)}\n')
            idx += 1

            if cfg.eval.max_iterations > 0 and idx >= cfg.eval.max_iterations:
                break

    # ì¼ë°˜ì§ˆë¬¸ eval_id ë¦¬ìŠ¤íŠ¸ì™€ ê°¯ìˆ˜ ë¡œê·¸ì— ì¶œë ¥
    log.info(f'ì¼ë°˜ì§ˆë¬¸ eval_id ë¦¬ìŠ¤íŠ¸ ({len(general_eval_ids)}ê°œ): {general_eval_ids}')

    # chit_chat_idsë¥¼ ì œì™¸í•œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë¡œê·¸ì— ì¶œë ¥
    chit_chat_ids = {2, 32, 57, 64, 67, 83, 90, 94, 103, 218, 220, 222, 227, 229, 245, 247, 261, 276, 283, 301}
    filtered_eval_ids = [eid for eid in general_eval_ids if eid not in chit_chat_ids]
    log.info(f'chit_chat_ids ì œì™¸ ìµœì¢… eval_id ë¦¬ìŠ¤íŠ¸ ({len(filtered_eval_ids)}ê°œ): {filtered_eval_ids}')

@hydra.main(config_path="conf", config_name="config", version_base=None)
def main(cfg: DictConfig) -> None:
    log = logging.getLogger(__name__)
    log.info("Starting RAG evaluation process")
    
    # LLM API í‚¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸ (ëª¨ë¸ì— ë”°ë¼)
    model_name = cfg.llm.model.lower()
    if "gemini" in model_name:
        if not (os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")):
            raise ValueError("Gemini ëª¨ë¸ ì‚¬ìš©ì‹œ GEMINI_API_KEY ë˜ëŠ” GOOGLE_API_KEY environment variable is required")
    else:
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OPENAI_API_KEY environment variable is required")

    # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± (OpenAI ë˜ëŠ” Gemini)
    client = create_llm_client(cfg)

    # Elasticsearch ì„¤ì •
    es_password = os.getenv("ELASTICSEARCH_PASSWORD")
    if not es_password:
        raise ValueError("ELASTICSEARCH_PASSWORD environment variable is required")

    # Elasticsearch client ìƒì„±
    es = Elasticsearch(
        list(cfg.elasticsearch.hosts), 
        basic_auth=(cfg.elasticsearch.username, es_password), 
        ca_certs=cfg.elasticsearch.ca_certs
    )
    log.info(f'Elasticsearch connection established: {es.info()}')

    # Retrieve ë°±ì—”ë“œ í™œì„±í™” ì—¬ë¶€
    upstage_enabled = getattr(cfg.retrieve.dense_upstage, 'enabled', False)
    sbert_enabled = getattr(cfg.retrieve.dense_sbert, 'enabled', False)
    upstage_hyde_enabled = getattr(cfg.retrieve.dense_upstage_hyde, 'enabled', False)
    gemini_enabled = getattr(cfg.retrieve.dense_gemini, 'enabled', False)
    gemini_hyde_enabled = getattr(cfg.retrieve.dense_gemini_hyde, 'enabled', False)

    # ë°±ì—”ë“œë³„ ëª¨ë¸ ì´ˆê¸°í™” (í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
    # upstage ë˜ëŠ” upstage_hydeê°€ í™œì„±í™”ëœ ê²½ìš° solar_model í•„ìš”
    solar_model = UpstageEmbeddings(model=cfg.retrieve.dense_upstage.model_name) if (upstage_enabled or upstage_hyde_enabled) else None
    sbert_model = SentenceTransformer(cfg.retrieve.dense_sbert.model_name) if sbert_enabled else None

    # Gemini ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (gemini ë˜ëŠ” gemini_hydeê°€ í™œì„±í™”ëœ ê²½ìš°)
    gemini_model = None
    if gemini_enabled or gemini_hyde_enabled:
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError("Gemini ì„ë² ë”© ì‚¬ìš©ì‹œ GOOGLE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        gemini_model = GoogleGenerativeAIEmbeddings(
            model=cfg.retrieve.dense_gemini.model_name,
            google_api_key=google_api_key
        )
        log.info(f"Gemini ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”: {cfg.retrieve.dense_gemini.model_name}")

    # Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •
    # Elasticsearch 9.xì—ì„œëŠ” Noriì˜ í’ˆì‚¬ íƒœê·¸ê°€ ì„¸ë¶„í™”ë˜ì–´
    # ê¸°ì¡´ì˜ "E", "J" ë“±ì˜ ëŒ€ë¶„ë¥˜ íƒœê·¸ê°€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # ì•„ë˜ëŠ” ë™ì¼ ì˜ë¯¸ì˜ ì„¸ë¶„í™”ëœ íƒœê·¸ë¡œ êµì²´í•œ ëª©ë¡ì…ë‹ˆë‹¤.
    # - E(ì–´ë¯¸): EC, EF, EP, ETN, ETM
    # - J(ì¡°ì‚¬): JKS, JKC, JKG, JKO, JKB, JKV, JKQ, JX, JC
    # - ê¸°í˜¸/êµ¬ë‘ì : SC, SE, SF, SP, SSO, SSC, SY
    # - ë³´ì¡° ìš©ì–¸/ê³„ì‚¬: VCP, VCN, VX
    settings = {
        "analysis": {
            "analyzer": {
                "nori": {
                    "type": "custom",
                    "tokenizer": "nori_tokenizer",
                    "decompound_mode": "mixed",
                    "filter": ["nori_posfilter"]
                }
            },
            "filter": {
                "nori_posfilter": {
                    "type": "nori_part_of_speech",
                    "stoptags": [
                        # E (ì–´ë¯¸)
                        "EC", "EF", "EP", "ETN", "ETM",
                        # J (ì¡°ì‚¬)
                        "JKS", "JKC", "JKG", "JKO", "JKB", "JKV", "JKQ", "JX", "JC",
                        # ê¸°í˜¸/êµ¬ë‘ì 
                        "SC", "SE", "SF", "SP", "SSO", "SSC", "SY",
                        # ë³´ì¡° ìš©ì–¸/ê³„ì‚¬
                        "VCP", "VCN", "VX"
                    ]
                }
            }
        }
    }

    # í™œì„±í™”ëœ dense retrieve ë°©ì‹ì— ë”°ë¼ ë§¤í•‘ ë™ì  ìƒì„±
    mappings = {
        "properties": {
            "content": {"type": "text", "analyzer": "nori"}
        }
    }

    # Upstage ì„ë² ë”©ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ í•„ë“œ ì¶”ê°€ (ì¼ë°˜ upstage ë˜ëŠ” hyde ë°©ì‹ ëª¨ë‘ ë™ì¼í•œ í•„ë“œ ì‚¬ìš©)
    if upstage_enabled or upstage_hyde_enabled:
        mappings["properties"]["embeddings_upstage"] = {
            "type": "dense_vector",
            "dims": 4096,
            "index": True,
            "similarity": "l2_norm"
        }

    # SBERT ì„ë² ë”©ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ í•„ë“œ ì¶”ê°€
    if sbert_enabled:
        mappings["properties"]["embeddings_sbert"] = {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "l2_norm"
        }

    # Gemini ì„ë² ë”©ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ í•„ë“œ ì¶”ê°€ (3072ì°¨ì›)
    if gemini_enabled or gemini_hyde_enabled:
        mappings["properties"]["embeddings_gemini"] = {
            "type": "dense_vector",
            "dims": 3072,
            "index": True,
            "similarity": "l2_norm"
        }

    # ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    force_recreate = getattr(cfg.index, 'force_recreate', True)
    index_exists = es.indices.exists(index=cfg.index.name)
    create_es_index(es, cfg.index.name, settings, mappings, force_recreate)

    # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ê³  force_recreateê°€ Falseë©´ ì„ë² ë”© ë° ìƒ‰ì¸ ê±´ë„ˆë›°ê¸°
    if index_exists and not force_recreate:
        log.info(f'ê¸°ì¡´ ì¸ë±ìŠ¤ "{cfg.index.name}" ì¬ì‚¬ìš© - ì„ë² ë”© ë° ìƒ‰ì¸ ê³¼ì • ê±´ë„ˆëœ€')
    else:
        # ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹±
        index_docs = []
        with open(cfg.paths.documents) as f:
            docs = [json.loads(line) for line in f]

        # Upstage(4096 ì°¨ì› ê·¸ëŒ€ë¡œ ì‚¬ìš©) - ì¼ë°˜ upstage ë˜ëŠ” hyde ë°©ì‹ ëª¨ë‘ ë™ì¼í•œ ì„ë² ë”© ì‚¬ìš©
        solar_embeds = None
        if (upstage_enabled or upstage_hyde_enabled) and solar_model is not None:
            solar_embeds = get_embeddings_in_batches(docs, solar_model, cfg.embedding.batch_size)

        # SBERT(768)
        sbert_embeds = None
        if sbert_enabled and sbert_model is not None:
            sbert_embeds = []
            bs = cfg.embedding.batch_size
            for i in range(0, len(docs), bs):
                batch = docs[i:i+bs]
                contents = [d["content"] for d in batch]
                sbert_embeds.extend(sbert_get_embedding(contents, sbert_model))

        # Gemini(3072) - ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒì„±ëœ ì„ë² ë”© ë¡œë“œ
        gemini_embeds = None
        if (gemini_enabled or gemini_hyde_enabled) and gemini_model is not None:
            try:
                from gemini_embedding_generator import GeminiEmbeddingGenerator
                generator = GeminiEmbeddingGenerator(cfg)
                gemini_embeds = generator.get_all_embeddings()

                if gemini_embeds is None:
                    log.warning("Gemini ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
                    log.warning("python gemini_embedding_generator.py")
                    log.warning("Gemini ì„ë² ë”© ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                    # Gemini ê¸°ëŠ¥ ë¹„í™œì„±í™”
                    gemini_enabled = False
                    gemini_hyde_enabled = False
                else:
                    log.info(f"Gemini ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(gemini_embeds)}ê°œ")
            except Exception as e:
                log.error(f"Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
                log.warning("Gemini ì„ë² ë”© ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                gemini_enabled = False
                gemini_hyde_enabled = False
                gemini_embeds = None

        # ë¬¸ì„œì— í•„ìš”í•œ í•„ë“œë§Œ ì¶”ê°€í•˜ì—¬ ìƒ‰ì¸
        for idx, doc in enumerate(docs):
            if solar_embeds is not None:
                vec = solar_embeds[idx]
                if hasattr(vec, 'tolist'):
                    vec = vec.tolist()
                doc["embeddings_upstage"] = vec
            if sbert_embeds is not None:
                vec = sbert_embeds[idx]
                if hasattr(vec, 'tolist'):
                    vec = vec.tolist()
                doc["embeddings_sbert"] = vec
            if gemini_embeds is not None:
                vec = gemini_embeds[idx]
                if hasattr(vec, 'tolist'):
                    vec = vec.tolist()
                doc["embeddings_gemini"] = vec
            index_docs.append(doc)

        ret = bulk_add(es, cfg.index.name, index_docs)
        log.info(f'Bulk indexing completed: {ret}')

        # ìƒ‰ì¸ ì™„ë£Œ í›„ ë¶ˆí•„ìš”í•œ ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í•´ì œ (ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ ì „ OOM ë°©ì§€)
        import gc
        log.info('ìƒ‰ì¸ ì™„ë£Œ - ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ í•´ì œ ì‹œì‘')

        # ëŒ€ìš©ëŸ‰ ë³€ìˆ˜ë“¤ ë©”ëª¨ë¦¬ í•´ì œ
        if 'solar_embeds' in locals() and solar_embeds is not None:
            del solar_embeds
        if 'sbert_embeds' in locals() and sbert_embeds is not None:
            del sbert_embeds
        if 'gemini_embeds' in locals() and gemini_embeds is not None:
            del gemini_embeds
        if 'index_docs' in locals():
            del index_docs
        if 'docs' in locals():
            del docs

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        gc.collect()

        # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (PyTorch ì‚¬ìš© ì‹œ)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        log.info('ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ - ë¦¬ë­í‚¹ ëª¨ë¸ ë¡œë“œ ì¤€ë¹„')

    # í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ìƒì„±
    # CSV íŒŒì¼ì„ Hydra outputs ë””ë ‰í† ë¦¬ì— ì €ì¥
    from hydra.core.hydra_config import HydraConfig
    hydra_output_dir = HydraConfig.get().runtime.output_dir
    output_path = os.path.join(hydra_output_dir, cfg.paths.output)
    
    log.info(f'Current working directory: {os.getcwd()}')
    log.info(f'Hydra output directory: {hydra_output_dir}')
    log.info(f'Starting evaluation with output file: {output_path}')
    dense_ctx = {
        'upstage': {'model': solar_model} if upstage_enabled and solar_model is not None else None,
        'sbert': {'model': sbert_model} if sbert_enabled and sbert_model is not None else None,
        'upstage_hyde': {'model': solar_model} if upstage_hyde_enabled and solar_model is not None else None,
        'gemini': {'model': gemini_model} if gemini_enabled and gemini_model is not None else None,
        'gemini_hyde': {'model': gemini_model} if gemini_hyde_enabled and gemini_model is not None else None,
    }
    eval_rag(cfg.paths.eval_data, output_path, client, cfg, es, cfg.index.name, dense_ctx)
    log.info('RAG evaluation process completed')


if __name__ == "__main__":
    main()
