import os
import json
from dotenv import load_dotenv
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Sentence Transformer ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ì„ë² ë”© ìƒì„± ê°€ëŠ¥í•œ ì–´ë–¤ ëª¨ë¸ë„ ê°€ëŠ¥)
model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")


# SetntenceTransformerë¥¼ ì´ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
def get_embedding(sentences):
    return model.encode(sentences)


# ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
def get_embeddings_in_batches(docs, batch_size=100):
    batch_embeddings = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        contents = [doc["content"] for doc in batch]
        embeddings = get_embedding(contents)
        batch_embeddings.extend(embeddings)
        print(f'batch {i}')
    return batch_embeddings


# ìƒˆë¡œìš´ index ìƒì„±
def create_es_index(index, settings, mappings):
    # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if es.indices.exists(index=index):
        # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì„¤ì •ì„ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ ê°±ì‹ í•˜ê¸° ìœ„í•´ ì‚­ì œ
        es.indices.delete(index=index)
    # ì§€ì •ëœ ì„¤ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
    es.indices.create(index=index, settings=settings, mappings=mappings)


# ì§€ì •ëœ ì¸ë±ìŠ¤ ì‚­ì œ
def delete_es_index(index):
    es.indices.delete(index=index)


# Elasticsearch í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ ì¸ë±ì‹± ìˆ˜í–‰
def bulk_add(index, docs):
    # ëŒ€ëŸ‰ ì¸ë±ì‹± ì‘ì—…ì„ ì¤€ë¹„
    actions = [
        {
            '_index': index,
            '_source': doc
        }
        for doc in docs
    ]
    return helpers.bulk(es, actions)


# ì—­ìƒ‰ì¸ì„ ì´ìš©í•œ ê²€ìƒ‰
def sparse_retrieve(query_str, size):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    return es.search(index="test", query=query, size=size, sort="_score")


# Vector ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ê²€ìƒ‰
def dense_retrieve(query_str, size):
    # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì¿¼ë¦¬ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
    query_embedding = get_embedding([query_str])[0]

    # KNNì„ ì‚¬ìš©í•œ ë²¡í„° ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
    knn = {
        "field": "embeddings",
        "query_vector": query_embedding.tolist(),
        "k": size,
        "num_candidates": 100
    }

    # ì§€ì •ëœ ì¸ë±ìŠ¤ì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
    return es.search(index="test", knn=knn)


es_username = "elastic"
es_password = os.getenv("ELASTICSEARCH_PASSWORD")
if not es_password:
    raise ValueError("ELASTICSEARCH_PASSWORD environment variable is required")

# Elasticsearch client ìƒì„±
es = Elasticsearch(['https://localhost:9200'], basic_auth=(es_username, es_password), ca_certs="/data/ephemeral/home/elasticsearch-8.8.0/config/certs/http_ca.crt")

# Elasticsearch client ì •ë³´ í™•ì¸
print(es.info())

# ìƒ‰ì¸ì„ ìœ„í•œ setting ì„¤ì •
settings = {
    "analysis": {
        "analyzer": {
            "nori": {
                "type": "custom",
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",
                "filter": ["nori_posfilter"]
            }
        },
        "filter": {
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                # ì–´ë¯¸, ì¡°ì‚¬, êµ¬ë¶„ì, ì¤„ì„í‘œ, ì§€ì •ì‚¬, ë³´ì¡° ìš©ì–¸ ë“±
                "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
            }
        }
    }
}

# ìƒ‰ì¸ì„ ìœ„í•œ mapping ì„¤ì • (ì—­ìƒ‰ì¸ í•„ë“œ, ì„ë² ë”© í•„ë“œ ëª¨ë‘ ì„¤ì •)
mappings = {
    "properties": {
        "content": {"type": "text", "analyzer": "nori"},
        "embeddings": {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "l2_norm"
        }
    }
}

# settings, mappings ì„¤ì •ëœ ë‚´ìš©ìœ¼ë¡œ 'test' ì¸ë±ìŠ¤ ìƒì„±
create_es_index("test", settings, mappings)

# ë¬¸ì„œì˜ content í•„ë“œì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
index_docs = []
with open("../../input/data/documents.jsonl") as f:
    docs = [json.loads(line) for line in f]
embeddings = get_embeddings_in_batches(docs)
                
# ìƒì„±í•œ ì„ë² ë”©ì„ ìƒ‰ì¸í•  í•„ë“œë¡œ ì¶”ê°€
for doc, embedding in zip(docs, embeddings):
    doc["embeddings"] = embedding.tolist()
    index_docs.append(doc)

# 'test' ì¸ë±ìŠ¤ì— ëŒ€ëŸ‰ ë¬¸ì„œ ì¶”ê°€
ret = bulk_add("test", index_docs)

# ìƒ‰ì¸ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ìƒ‰ì¸ëœ ì´ ë¬¸ì„œìˆ˜ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨)
print(ret)

test_query = "ê¸ˆì„±ì´ ë‹¤ë¥¸ í–‰ì„±ë“¤ë³´ë‹¤ ë°ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

# ì—­ìƒ‰ì¸ì„ ì‚¬ìš©í•˜ëŠ” ê²€ìƒ‰ ì˜ˆì œ
search_result_retrieve = sparse_retrieve(test_query, 3)

# ê²°ê³¼ ì¶œë ¥ í…ŒìŠ¤íŠ¸
for rst in search_result_retrieve['hits']['hits']:
    print('score:', rst['_score'], 'source:', rst['_source']["content"])

# Vector ìœ ì‚¬ë„ ì‚¬ìš©í•œ ê²€ìƒ‰ ì˜ˆì œ
search_result_retrieve = dense_retrieve(test_query, 3)

# ê²°ê³¼ ì¶œë ¥ í…ŒìŠ¤íŠ¸
for rst in search_result_retrieve['hits']['hits']:
    print('score:', rst['_score'], 'source:', rst['_source']["content"])


# ì•„ë˜ë¶€í„°ëŠ” ì‹¤ì œ RAGë¥¼ êµ¬í˜„í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
from openai import OpenAI
import traceback

# OpenAI API í‚¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY environment variable is required")

# OpenAI Base URLê³¼ Model í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
openai_base_url = os.getenv("OPENAI_BASE_URL")
openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # ê¸°ë³¸ê°’: gpt-4o-mini

# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (base_urlì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©)
if openai_base_url:
    client = OpenAI(base_url=openai_base_url)
else:
    client = OpenAI()

# ì‚¬ìš©í•  ëª¨ë¸ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •
llm_model = openai_model

# RAG êµ¬í˜„ì— í•„ìš”í•œ Question Answeringì„ ìœ„í•œ LLM  í”„ë¡¬í”„íŠ¸
persona_qa = """
## Role: ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€

## Instructions
- ì‚¬ìš©ìì˜ ì´ì „ ë©”ì‹œì§€ ì •ë³´ ë° ì£¼ì–´ì§„ Reference ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
- ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ë‹µì„ í•  ìˆ˜ ì—†ë‹¤ê³  ëŒ€ë‹µí•œë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
"""

# RAG êµ¬í˜„ì— í•„ìš”í•œ ì§ˆì˜ ë¶„ì„ ë° ê²€ìƒ‰ ì´ì™¸ì˜ ì¼ë°˜ ì§ˆì˜ ëŒ€ì‘ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸
persona_function_calling = """
## Role: ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€

## Instruction
- ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ í†µí•´ ê³¼í•™ ì§€ì‹ì— ê´€í•œ ì£¼ì œë¡œ ì§ˆë¬¸í•˜ë©´ search apië¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
- ê³¼í•™ ìƒì‹ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ëŒ€í™” ë©”ì‹œì§€ì—ëŠ” ì ì ˆí•œ ëŒ€ë‹µì„ ìƒì„±í•œë‹¤.
"""

# Function callingì— ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜
tools = [
    {
        "type": "function",
        "function": {
            "name": "search",
            "description": "search relevant documents",
            "parameters": {
                "properties": {
                    "standalone_query": {
                        "type": "string",
                        "description": "Final query suitable for use in search from the user messages history."
                    }
                },
                "required": ["standalone_query"],
                "type": "object"
            }
        }
    },
]


# LLMê³¼ ê²€ìƒ‰ì—”ì§„ì„ í™œìš©í•œ RAG êµ¬í˜„
def answer_question(messages):
    # í•¨ìˆ˜ ì¶œë ¥ ì´ˆê¸°í™”
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    # ì§ˆì˜ ë¶„ì„ ë° ê²€ìƒ‰ ì´ì™¸ì˜ ì§ˆì˜ ëŒ€ì‘ì„ ìœ„í•œ LLM í™œìš©
    msg = [{"role": "system", "content": persona_function_calling}] + messages
    try:
        result = client.chat.completions.create(
            model=llm_model,
            messages=msg,
            tools=tools,
            #tool_choice={"type": "function", "function": {"name": "search"}},
            temperature=0,
            seed=1,
            timeout=10
        )
    except Exception as e:
        traceback.print_exc()
        return response

    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ê²€ìƒ‰ í˜¸ì¶œí›„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    if result.choices[0].message.tool_calls:
        tool_call = result.choices[0].message.tool_calls[0]
        function_args = json.loads(tool_call.function.arguments)
        standalone_query = function_args.get("standalone_query")

        # Baselineìœ¼ë¡œëŠ” sparse_retrieveë§Œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
        search_result = sparse_retrieve(standalone_query, 3)

        response["standalone_query"] = standalone_query
        retrieved_context = []
        for i,rst in enumerate(search_result['hits']['hits']):
            retrieved_context.append(rst["_source"]["content"])
            response["topk"].append(rst["_source"]["docid"])
            response["references"].append({"score": rst["_score"], "content": rst["_source"]["content"]})

        content = json.dumps(retrieved_context)
        messages.append({"role": "assistant", "content": content})
        msg = [{"role": "system", "content": persona_qa}] + messages
        try:
            qaresult = client.chat.completions.create(
                    model=llm_model,
                    messages=msg,
                    temperature=0,
                    seed=1,
                    timeout=30
                )
        except Exception as e:
            traceback.print_exc()
            return response
        response["answer"] = qaresult.choices[0].message.content

    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ë°”ë¡œ ë‹µë³€ ìƒì„±
    else:
        response["answer"] = result.choices[0].message.content

    return response


# í‰ê°€ë¥¼ ìœ„í•œ íŒŒì¼ì„ ì½ì–´ì„œ ê° í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ì¶”ì¶œí›„ íŒŒì¼ì— ì €ì¥
def eval_rag(eval_filename, output_filename):
    with open(eval_filename) as f, open(output_filename, "w") as of:
        idx = 0
        for line in f:
            j = json.loads(line)
            print(f'ğŸš©Test {idx}\nQuestion: {j["msg"]}')
            response = answer_question(j["msg"])
            print(f'Answer: {response["answer"]}\n')

            # ëŒ€íšŒ score ê³„ì‚°ì€ topk ì •ë³´ë¥¼ ì‚¬ìš©, answer ì •ë³´ëŠ” LLMì„ í†µí•œ ìë™í‰ê°€ì‹œ í™œìš©
            output = {"eval_id": j["eval_id"], "standalone_query": response["standalone_query"], "topk": response["topk"], "answer": response["answer"], "references": response["references"]}
            of.write(f'{json.dumps(output, ensure_ascii=False)}\n')
            idx += 1

# í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ ìƒì„± - íŒŒì¼ í¬ë§·ì€ jsonlì´ì§€ë§Œ íŒŒì¼ëª…ì€ csv ì‚¬ìš©
eval_rag("../../input/data/eval.jsonl", "sample_submission.csv")

