## Slide 4

- **Team 4조** - 과학 지식 질의 응답 시스템 구축
- 팀장
  - **류지헌** - RAG 아키텍처 설계, RAG 파이프라인 구현
- 팀원
  - **김태현** - 문서 전처리 및 분할, 검색 최적화
- 팀원
  - **문진숙** - 프롬프트 엔지니어링, 답변 품질 개선
- 팀원
  - **박진섭** - 임베딩 및 벡터 저장소, 성능 튜닝
- 팀원
  - **김재덕** - API 통합 및 배포, 환경 설정 관리


## Slide 5
### 프로젝트 협업 방식
  - **GitHub Issues 기반 협업**: 체계적인 이슈 관리와 진행 상황 추적
  - **정기 미팅**: 매일 1회 이상 정기 미팅으로 진행 상황 공유
  - **역할 분담**: 각자 전문 분야에 집중하여 효율적인 개발 진행
  - **실시간 소통**: 모르는 것은 바로바로 질문하고 해결 (메신저 활용)
  - **코드 리뷰**: 서로의 코드를 검토하여 품질 향상
  - **문서화**: README, 노트, 설정 파일을 통한 지식 공유


## Slide 7

### 주제
* **Information Retrieval 경진대회** - 과학 지식 질의 응답 시스템 구축
* **팀 주제**: RAG(Retrieval-Augmented Generation) 기반 과학 지식 QA 시스템

### 목표
* **목표**
  - 질문과 이전 대화 히스토리를 바탕으로 검색엔진에서 관련 문서를 추출
  - 추출된 문서를 활용해 적합한 답변을 생성하는 RAG 시스템 구축
* **주요 작업**
  - 하이브리드 검색 (Sparse + Dense) 구현
  - HyDE(Hypothetical Document Embeddings) 기법 적용
  - 하드보팅/리랭커 앙상블 시스템 구축
  - 멀티 임베딩 백엔드 통합 (Upstage, SBERT, Gemini)

### 개요
* **소개 및 배경 설명**
  - 과학 지식 질의 응답을 위한 고성능 RAG 시스템 개발
  - 4,272개 과학 문서 코퍼스와 220개 평가 질의 세트 활용
* **기간**
  - 2025. 09. 08 ~ 2025. 09. 18 (10일간)


## Slide 8
### 프로젝트 개요 - 기술 스택 및 아키텍처 설계

**기술 스택**
- **검색엔진**: Elasticsearch 9.x (한국어 nori 분석기)
- **임베딩 모델**: Upstage(4096d), SBERT(768d), Gemini(3072d)
- **LLM**: Solar-Pro2, GPT-4o-mini, Gemini 2.5 Flash
- **프레임워크**: Hydra, LangChain, Sentence-Transformers
- **언어**: Python 3.10+, uv 패키지 관리

**아키텍처 특징**
- 하이브리드 검색: Sparse(BM25) + Dense(멀티 임베딩)
- HyDE 기법: 가상 문서 생성으로 검색 품질 향상
- 앙상블 시스템: 하드보팅 + 리랭커 조합
- 캐시 시스템: LLM/임베딩 결과 디스크 캐시


## Slide 10
### 프로젝트 수행 절차 및 방법 - 작품 소개

**프로젝트명**: 과학 지식 질의 응답 시스템 구축

**개발 배경**
- 과학 지식에 대한 정확하고 신뢰할 수 있는 답변 제공 필요
- 기존 단순 검색의 한계를 극복하기 위한 RAG 시스템 필요
- 다양한 검색 방식의 장점을 결합한 하이브리드 접근법 필요

**목적 및 목표**
- 질문 유형 자동 분류 (과학 질문 vs 일상 대화)
- 관련 문서 정확한 검색 및 랭킹
- 검색된 문서 기반 고품질 답변 생성
- 최고 성능: MAP 0.9424, MRR 0.9439 달성


## Slide 11
### 프로젝트 수행 절차 및 방법 - 구현 전략

**시스템 구조 개요**
1. **질의 분석**: Function Calling으로 standalone_query 생성
2. **하이브리드 검색**: Sparse(BM25) + Dense(멀티 임베딩) 병렬 검색
3. **HyDE 적용**: 가상 문서 생성으로 검색 품질 향상
4. **앙상블 랭킹**: 하드보팅 또는 리랭커로 최종 문서 선택
5. **답변 생성**: 선택된 문서 기반 LLM 답변 생성

**프롬프트 엔지니어링 전략**
- Function Calling 프롬프트로 chit-chat 20개 자동 제외
- HyDE 프롬프트로 가상 문서 생성 (2가지 버전)
- 리랭커 프롬프트로 문서 관련성 평가

**데이터 처리 및 연동**
- Elasticsearch 멀티 인덱스 관리 (sparse, upstage, sbert, gemini)
- Query Embedding 캐시 시스템으로 API 비용 절감
- Hydra 설정 기반 실험 관리 및 파라미터화


## Slide 12
### 프로젝트 수행 절차 및 방법 - 기능 리뷰

**기능1: 하이브리드 검색 시스템**
- Sparse 검색: BM25 + 한국어 nori 분석기로 키워드 기반 검색
- Dense 검색: 3가지 임베딩 모델(Upstage, SBERT, Gemini) 병렬 검색
- 검색 결과 병합: 각 방식별 top-k 문서를 통합하여 다양성 확보

**기능2: HyDE(Hypothetical Document Embeddings)**
- 가상 문서 생성: 질문에 대한 가상 답변을 생성하여 검색 품질 향상
- 2가지 프롬프트 버전: 일반적 답변 vs 구체적 예시 포함 답변
- 검색 정확도 향상: MAP 0.8197 → 0.8985 (Upstage 기준)

**기능3: 앙상블 랭킹 시스템**
- 하드보팅: 여러 검색 결과의 중복도 기반 랭킹 (5:3:1 가중치)
- 리랭커: CausalLM 기반 yes/no 스코어링으로 문서 관련성 평가
- 최고 성능: 하드보팅 CSV 앙상블 (MAP: 0.9424, MRR: 0.9439)

**기능4: Function Calling 기반 질문 분류**
- chit-chat 20개 자동 제외: 일상 대화와 과학 질문 자동 구분
- standalone_query 생성: 멀티턴 대화에서 핵심 질문 추출


## Slide 13
### 프로젝트 수행 절차 및 방법 - 트러블 슈팅

**문제 1: Function Calling 프롬프트 최적화**
- 문제: chit-chat과 과학 질문 구분 정확도가 낮음 (57개 오분류)
- 해결: 4번의 프롬프트 개선을 통해 21개까지 감소
- 결과: MAP 0.3402 → 0.8720으로 대폭 향상

**문제 2: 메모리 부족 및 차원 불일치**
- 문제: Qwen-embedding 8B 모델 사용 시 CUDA 메모리 부족
- 해결: `os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"` 적용
- 추가: 메모리 정리 및 배치 크기 조절로 안정성 확보

**문제 3: Elasticsearch 차원 제한**
- 문제: ES 8.x에서 768차원 제한으로 Upstage(4096d) 사용 불가
- 해결: ES 9.x 업그레이드로 차원 제한 해결
- 결과: PCA 차원 축소 없이 원본 임베딩 사용 가능

**문제 4: 검색 성능 최적화**
- 문제: 단일 검색 방식의 한계로 성능 정체
- 해결: 하이브리드 검색 + 앙상블 랭킹 도입
- 결과: 최적 top-k=40에서 MAP 0.9061 달성


## Slide 15
### 프로젝트 회고 - 결과 및 향후 계획

**최종 결과**
- **최고 성능**: MAP 0.9424, MRR 0.9439 달성
- **하이브리드 검색**: Sparse + Dense 조합으로 검색 품질 대폭 향상
- **앙상블 시스템**: 하드보팅(5:3:1) + 리랭커 조합으로 안정적 성능 확보
- **자동화**: Function Calling으로 chit-chat 자동 제외 (21개 오분류)

**도전 과제 및 해결**
- **프롬프트 엔지니어링**: 4번의 반복 개선으로 정확도 3배 향상
- **메모리 최적화**: CUDA 설정과 배치 처리로 대용량 모델 안정적 실행
- **차원 제한**: ES 9.x 업그레이드로 고차원 임베딩 활용 가능

**인사이트 도출**
- **하이브리드 접근법의 중요성**: 단일 방식보다 다양한 검색 방식 조합이 효과적
- **HyDE의 위력**: 가상 문서 생성이 검색 품질 향상에 핵심적 역할
- **앙상블의 안정성**: 개별 모델의 한계를 앙상블로 보완 가능

**개선 방향**
- **실시간 성능**: 캐시 시스템 확장으로 응답 속도 개선
- **모델 다양화**: 더 많은 임베딩 모델과 LLM 조합 실험
- **평가 체계**: 다양한 평가 메트릭 도입으로 성능 측정 정교화


## Slide 16
### 프로젝트 진행 소감 - 느낀점

**류지헌 (팀장)**
- 하이브리드 RAG 아키텍처 설계와 Hydra 기반 파이프라인 구현을 통해 검색 품질 향상의 핵심을 체감했습니다. Sparse/Dense 검색의 조합, HyDE 기법, 하드보팅/리랭커 앙상블을 통한 성능 개선 과정에서 팀원들과의 협업이 큰 도움이 되었습니다. 캐시 시스템과 메모리 관리 최적화로 운영 안정성도 크게 향상시킬 수 있었습니다.

**김태현**
- HyDE 프롬프트 엔지니어링과 하드보팅/리랭커 전략 수립에 기여했습니다. 다양한 검색 백엔드의 결과를 효과적으로 결합하는 방법을 연구하고, 하드보팅/리랭커의 스코어링 방식을 최적화했습니다. 실험 결과를 바탕으로 최적의 앙상블 전략을 도출할 수 있었습니다.

**문진숙**
- 한국어 nori 분석기와 멀티 임베딩 백엔드(Upstage/SBERT/Gemini) 조합을 통한 검색 성능 최적화에 집중했습니다. 문서 전처리와 인덱스 분리 전략이 검색 정확도에 미치는 영향을 체계적으로 분석하고, Hydra 설정을 통한 실험 관리 효율성을 크게 개선할 수 있었습니다.

**박진섭**
- 통합 LLM 호출 레이어와 캐시 시스템 구축, 환경 설정 표준화에 집중했습니다. OpenAI 호환/Gemini API 통합, 디스크 캐시 시스템, FlashAttention 환경 설정 등을 통해 개발/운영 효율성을 크게 향상시켰습니다. Hydra 기반 설정 관리와 문서화를 통해 팀 온보딩 시간을 단축할 수 있었습니다.

**김재덕**
- Elasticsearch 9.x 기반 멀티 인덱스 관리와 임베딩 최적화에 집중했습니다. ANN/Exact 검색 모드 전환, Query Embedding 캐시 시스템 구축을 통해 성능과 비용 효율성을 동시에 개선했습니다. Gemini 임베딩 사전 생성 파이프라인을 구축해 API 호출 비용을 크게 절감할 수 있었습니다.
